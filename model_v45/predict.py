# predict.py - –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏ "nauslava/rubber-tile-lora-v45"
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ù–ê–®–£ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å LoRA –∏ Textual Inversion

import os
import torch
import random
import gc
from typing import Optional
from PIL import Image
import numpy as np

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
os.environ["HF_HOME"] = "/tmp/hf_home"
os.environ["HF_DATASETS_CACHE"] = "/tmp/hf_datasets_cache"
os.environ["HF_MODELS_CACHE"] = "/tmp/hf_models_cache"
os.environ["TRANSFORMERS_CACHE_MIGRATION_DISABLE"] = "1"
os.environ["HF_HUB_CACHE_MIGRATION_DISABLE"] = "1"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler
from transformers import CLIPTextModel, T5EncoderModel

class Predictor:
    def __init__(self):
        self.device = None
        self.pipe = None
        self.setup()
    
    def setup(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞."""
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ v45...")
        
        # 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if torch.cuda.is_available():
            self.device = "cuda"
            # –í—ã–±–æ—Ä GPU —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –ø–∞–º—è—Ç—å—é
            best_gpu = max(range(torch.cuda.device_count()), 
                          key=lambda i: torch.cuda.get_device_properties(i).total_memory)
            torch.cuda.set_device(best_gpu)
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name(best_gpu)}")
        else:
            self.device = "cpu"
            print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ SDXL pipeline
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ SDXL...")
        self.pipe = StableDiffusionXLPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            use_safetensors=True,
            variant="fp16",
            resume_download=False
        )
        
        # 3. –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU
        self.pipe = self.pipe.to(self.device)
        
        # 4. –ó–∞–≥—Ä—É–∑–∫–∞ –ù–ê–®–ò–• –æ–±—É—á–µ–Ω–Ω—ã—Ö LoRA
        print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –ù–ê–®–ò–• LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
        lora_path = "/src/model_files/rubber-tile-lora-v4_sdxl_lora.safetensors"
        self.pipe.set_adapters(["rubber-tile-lora-v4_sdxl_lora"], adapter_weights=[0.7])
        self.pipe.fuse_lora()
        print("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã")
        
        # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –ù–ê–®–ò–• –æ–±—É—á–µ–Ω–Ω—ã—Ö Textual Inversion
        print("üî§ –ó–∞–≥—Ä—É–∑–∫–∞ –ù–ê–®–ò–• Textual Inversion...")
        ti_path = "/src/model_files/rubber-tile-lora-v4_sdxl_embeddings.safetensors"
        self.pipe.load_textual_inversion(ti_path, token="<s0>")
        print("‚úÖ Textual Inversion –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # 6. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
        print("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞...")
        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            self.pipe.scheduler.config,
            algorithm_type="dpmsolver++",
            use_karras_sigmas=True
        )
        
        # 7. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ VAE
        print("üöÄ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ VAE –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π...")
        self.pipe.vae.enable_slicing()
        self.pipe.vae.enable_tiling()
        
        # 8. –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        print("üéâ –ú–æ–¥–µ–ª—å v45 —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
    
    def _build_prompt(self, color_description: str) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ù–ê–®–ò–• –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤."""
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç —Å –ù–ê–®–ò–ú–ò —Ç–æ–∫–µ–Ω–∞–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        base_prompt = "ohwx_rubber_tile <s0><s1>"
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Ü–≤–µ—Ç–æ–≤
        full_prompt = f"{base_prompt}, {color_description}"
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤
        quality_descriptors = [
            "photorealistic rubber tile",
            "high quality",
            "detailed texture",
            "professional photography",
            "sharp focus"
        ]
        
        full_prompt += ", " + ", ".join(quality_descriptors)
        
        return full_prompt
    
    def predict(self, prompt: str, negative_prompt: Optional[str] = None, seed: int = -1) -> Image.Image:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–µ–∑–∏–Ω–æ–≤–æ–π –ø–ª–∏—Ç–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ù–ê–®–ï–ô –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if negative_prompt is None:
            negative_prompt = "blurry, worst quality, low quality, deformed, watermark, 3d render, cartoon, abstract, painting, drawing, text, sketch, low resolution"
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏–¥–∞
        if seed == -1:
            seed = random.randint(0, 999999999)
        
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –≤ –ø–æ–ª–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å –ù–ê–®–ò–ú–ò —Ç–æ–∫–µ–Ω–∞–º–∏
        full_prompt = self._build_prompt(prompt)
        
        print(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        print(f"üìù –ü—Ä–æ–º–ø—Ç: {full_prompt}")
        print(f"üö´ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {negative_prompt}")
        print(f"üé≤ –°–∏–¥: {seed}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        result = self.pipe(
            prompt=full_prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=20,
            guidance_scale=7.0,
            width=1024,
            height=1024,
            generator=torch.Generator(device=self.device).manual_seed(seed)
        )
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        print("‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ!")
        return result.images[0]
