# predict.py
from cog import BasePredictor, Input, Path
from typing import List, Dict, Any, Tuple, Optional
import json
import os
import random
import time
import logging
import warnings
import psutil
import threading

import numpy as np
from PIL import Image
import cv2

import torch
from safetensors.torch import load_file as load_safetensors
from diffusers import (
    StableDiffusionXLControlNetPipeline,
    ControlNetModel,
    EulerDiscreteScheduler,
)

# üöÄ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π - v4.3.7
import warnings
import gc  # –î–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π —Å–±–æ—Ä–∫–∏ –º—É—Å–æ—Ä–∞

# –û—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings("ignore", message=".*torch.utils._pytree.*")
# # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º pytree warnings —á–µ—Ä–µ–∑ monkey patching
# import torch.utils._pytree as _pytree
# if hasattr(_pytree, "_register_pytree_node") and not hasattr(_pytree, "_register_pytree_node_fixed"):
#     _pytree._register_pytree_node_fixed = _pytree._register_pytree_node
#     _pytree._register_pytree_node = _pytree.register_pytree_node
#     logger.info("üîß Pytree warnings –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã —á–µ—Ä–µ–∑ monkey patching")
warnings.filterwarnings("ignore", message=".*_register_pytree_node.*")
warnings.filterwarnings("ignore", message=".*Please use `torch.utils._pytree.register_pytree_node`.*")
warnings.filterwarnings("ignore", message=".*The `scheduler_config`.*")
warnings.filterwarnings("ignore", message=".*Loaded state dictonary is incorrect.*")
warnings.filterwarnings("ignore", message=".*Please verify that the loaded state dictionary.*")
warnings.filterwarnings("ignore", message=".*string_to_param.*")
warnings.filterwarnings("ignore", message=".*accelerate.*memory.*")
warnings.filterwarnings("ignore", message=".*cache.*migration.*")

# üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ö–≠–®–ê –ò –°–ò–°–¢–ï–ú–´ - v4.3.7
import os
os.environ['PYTHONWARNINGS'] = 'ignore'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# üöÄ –ù–û–í–´–ï: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫—ç—à–∞ –∏ –º–∏–≥—Ä–∞—Ü–∏–∏
os.environ['HF_HUB_CACHE'] = '/tmp/huggingface_cache'  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—É—Ç—å –∫—ç—à–∞
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'  # –û—Ç–∫–ª—é—á–∏—Ç—å —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é
os.environ['HF_HUB_OFFLINE'] = '0'  # –†–∞–∑—Ä–µ—à–∏—Ç—å –æ–Ω–ª–∞–π–Ω –∑–∞–≥—Ä—É–∑–∫—É
os.environ['TRANSFORMERS_CACHE'] = '/tmp/transformers_cache'  # –ö—ç—à transformers
os.environ['DIFFUSERS_CACHE'] = '/tmp/diffusers_cache'  # –ö—ç—à diffusers
# 
# # üöÄ –ù–û–í–û–ï: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ cache migration warnings
# os.environ["TRANSFORMERS_OFFLINE"] = "0"  # –†–∞–∑—Ä–µ—à–∏—Ç—å –æ–Ω–ª–∞–π–Ω –∑–∞–≥—Ä—É–∑–∫—É
# os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫
# logger.info("üîß Cache migration warnings –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è")

# üöÄ –ù–û–í–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –û–ö–†–£–ñ–ï–ù–ò–Ø –î–õ–Ø –£–ü–†–ê–í–õ–ï–ù–ò–Ø GPU/NPU (–¢–û–õ–¨–ö–û –î–õ–Ø –õ–û–ö–ê–õ–¨–ù–û–ô –†–ê–ó–†–ê–ë–û–¢–ö–ò)
# os.environ['CUDA_MEMORY_FRACTION'] = '0.7'  # –£–ë–†–ê–ù–û: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è
os.environ['CUDA_CACHE_DISABLE'] = '0'      # –í–∫–ª—é—á–∏—Ç—å –∫—ç—à CUDA –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

# üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è CUDA out of memory
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ GPU 0
os.environ['TORCH_CUDNN_V8_API_DISABLED'] = '1'  # –û—Ç–∫–ª—é—á–∏—Ç—å cuDNN v8 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '0'   # –Ø–≤–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å cuDNN v8

# üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø ACCELERATE MEMORY - v4.3.7
os.environ['ACCELERATE_USE_CPU'] = '1'  # –û—Ç–∫–ª—é—á–∏—Ç—å GPU acceleration
os.environ['ACCELERATE_USE_CPU_IF_AVAILABLE'] = '1'  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU –¥–ª—è Accelerate
os.environ['ACCELERATE_USE_CPU_FOR_GPU'] = '1'  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ CPU –¥–ª—è GPU –æ–ø–µ—Ä–∞—Ü–∏–π
os.environ['ACCELERATE_USE_CPU_FOR_GPU_IF_AVAILABLE'] = '1'  # CPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
os.environ['ACCELERATE_USE_CPU_FOR_GPU_IF_AVAILABLE_AND_CPU_AVAILABLE'] = '1'  # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
os.environ['ACCELERATE_USE_CPU_FOR_GPU_IF_AVAILABLE_AND_CPU_AVAILABLE_AND_GPU_NOT_AVAILABLE'] = '1'  # –¢—Ä–æ–π–Ω–∞—è –∑–∞—â–∏—Ç–∞

# üöÄ –ù–û–í–´–ï: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
os.environ['ACCELERATE_MEMORY_EFFICIENT_ATTENTION'] = '1'  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è attention
os.environ['ACCELERATE_GRADIENT_CHECKPOINTING'] = '1'  # –í–∫–ª—é—á–∏—Ç—å gradient checkpointing
os.environ['ACCELERATE_CPU_OFFLOAD'] = '1'  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π offload –Ω–∞ CPU –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
os.environ['ACCELERATE_SAVE_CPU_OFFLOAD'] = '1'  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å CPU offload —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        
# # üöÄ –ù–û–í–û–ï: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ accelerate memory warnings
# os.environ["ACCELERATE_MAX_MEMORY"] = "0:13GB"  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å –¥–æ 13GB –∏–∑ 14.6GB
# os.environ["ACCELERATE_MEMORY_FRACTION"] = "0.9"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 90% –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
# logger.info("ÔøΩÔøΩ Accelerate warnings –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è")

# üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU –¥–ª—è VAE
os.environ['VAE_USE_CPU'] = '1'  # –ù–û–í–û–ï: VAE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç CPU
os.environ['VAE_USE_CPU_FOR_DECODE'] = '1'  # –ù–û–í–û–ï: VAE decode –Ω–∞ CPU
os.environ['VAE_USE_CPU_FOR_ENCODE'] = '1'  # –ù–û–í–û–ï: VAE encode –Ω–∞ CPU
os.environ['ACCELERATE_NO_CUDA'] = '1'  # –ù–û–í–û–ï: –ó–∞–ø—Ä–µ—Ç–∏—Ç—å CUDA –≤ Accelerate
os.environ['ACCELERATE_MIXED_PRECISION'] = 'no'  # –ù–û–í–û–ï: –û—Ç–∫–ª—é—á–∏—Ç—å mixed precision

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# üöÄ –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
class ResourceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU/NPU —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏."""
    
    def __init__(self, device_info: Dict[str, Any]):
        self.device_info = device_info
        self.monitoring = False
        self.monitor_thread = None
        self.max_memory_usage = 0.0
        self.max_gpu_utilization = 0.0
        
    def start_monitoring(self):
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        if self.monitoring:
            return
            
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        logger.info("üöÄ Resource monitoring started")
    
    def stop_monitoring(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        logger.info("‚èπÔ∏è Resource monitoring stopped")
    
    def _monitor_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        while self.monitoring:
            try:
                if self.device_info['type'] == 'cuda':
                    self._monitor_gpu()
                elif self.device_info['type'] == 'npu':
                    self._monitor_npu()
                
                time.sleep(2.0)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 2 —Å–µ–∫—É–Ω–¥—ã
            except Exception as e:
                logger.warning(f"Resource monitoring error: {e}")
                time.sleep(5.0)
    
    def _monitor_gpu(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU —Ä–µ—Å—É—Ä—Å–æ–≤."""
        try:
            if torch.cuda.is_available():
                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ GPU
                allocated = torch.cuda.memory_allocated(self.device_info['id']) / (1024**3)
                reserved = torch.cuda.memory_reserved(self.device_info['id']) / (1024**3)
                total = self.device_info['memory']
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                self.max_memory_usage = max(self.max_memory_usage, allocated)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤ (50-80%)
                memory_usage_percent = (allocated / total) * 100
                if memory_usage_percent > 80:
                    logger.warning(f"‚ö†Ô∏è GPU memory usage: {memory_usage_percent:.1f}% (allocated: {allocated:.2f}GB, reserved: {reserved:.2f}GB)")
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞
                    torch.cuda.empty_cache()
                elif memory_usage_percent > 70:
                    logger.info(f"‚ÑπÔ∏è GPU memory usage: {memory_usage_percent:.1f}% (allocated: {allocated:.2f}GB)")
                
                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≥—Ä—É–∑–∫–∏ GPU (—á–µ—Ä–µ–∑ nvidia-smi –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
                try:
                    import subprocess
                    result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                         capture_output=True, text=True, timeout=5)
                    if result.returncode == 0:
                        gpu_util = float(result.stdout.strip())
                        self.max_gpu_utilization = max(self.max_gpu_utilization, gpu_util)
                        
                        if gpu_util > 80:
                            logger.warning(f"‚ö†Ô∏è GPU utilization: {gpu_util:.1f}%")
                        elif gpu_util > 70:
                            logger.info(f"‚ÑπÔ∏è GPU utilization: {gpu_util:.1f}%")
                except:
                    pass  # nvidia-smi –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                    
        except Exception as e:
            logger.debug(f"GPU monitoring error: {e}")
    
    def _monitor_npu(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ NPU —Ä–µ—Å—É—Ä—Å–æ–≤."""
        try:
            # –î–ª—è NPU –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –∫–∞–∫ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            
            if cpu_percent > 80:
                logger.warning(f"‚ö†Ô∏è NPU system CPU usage: {cpu_percent:.1f}%")
            elif cpu_percent > 70:
                logger.info(f"‚ÑπÔ∏è NPU system CPU usage: {cpu_percent:.1f}%")
                
            if memory_percent > 80:
                logger.warning(f"‚ö†Ô∏è NPU system memory usage: {memory_percent:.1f}%")
            elif memory_percent > 70:
                logger.info(f"‚ÑπÔ∏è NPU system memory usage: {memory_percent:.1f}%")
                
        except Exception as e:
            logger.debug(f"NPU monitoring error: {e}")
    
    def get_resource_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ä–µ—Å—É—Ä—Å–æ–≤."""
        return {
            'device_type': self.device_info['type'],
            'device_name': self.device_info['name'],
            'max_memory_usage_gb': self.max_memory_usage,
            'max_gpu_utilization_percent': self.max_gpu_utilization,
            'monitoring_active': self.monitoring
        }

# üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –î–õ–Ø MULTI-GPU –ò NPU
def select_best_device():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (GPU/NPU/CPU)."""
    device_info = {
        'type': 'cpu',
        'id': None,
        'name': 'CPU',
        'memory': 0
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA GPU
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        logger.info(f"Found {gpu_count} CUDA GPU(s)")
        
        # –í—ã–±–æ—Ä GPU —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –ø–∞–º—è—Ç—å—é
        best_gpu = 0
        max_memory = 0
        
        for i in range(gpu_count):
            try:
                torch.cuda.set_device(i)
                props = torch.cuda.get_device_properties(i)
                memory_gb = props.total_memory / (1024**3)
                logger.info(f"GPU {i}: {props.name} - {memory_gb:.1f}GB")
                
                if memory_gb > max_memory:
                    max_memory = memory_gb
                    best_gpu = i
                    device_info = {
                        'type': 'cuda',
                        'id': i,
                        'name': props.name,
                        'memory': memory_gb
                    }
            except Exception as e:
                logger.warning(f"Failed to check GPU {i}: {e}")
        
        if device_info['type'] == 'cuda':
            torch.cuda.set_device(best_gpu)
            logger.info(f"‚úÖ Selected GPU {best_gpu}: {device_info['name']} ({device_info['memory']:.1f}GB)")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ NPU (Intel Neural Compute Stick, etc.)
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Intel NPU
        if os.path.exists('/dev/intel_npu0'):
            device_info = {
                'type': 'npu',
                'id': 0,
                'name': 'Intel NPU',
                'memory': 16  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å NPU
            }
            logger.info("‚úÖ Found Intel NPU")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—Ä—É–≥–∏—Ö NPU
        elif os.path.exists('/dev/npu0'):
            device_info = {
                'type': 'npu',
                'id': 0,
                'name': 'Generic NPU',
                'memory': 8
            }
            logger.info("‚úÖ Found Generic NPU")
    except Exception as e:
        logger.debug(f"NPU check failed: {e}")
    
    if device_info['type'] == 'cpu':
        logger.info("‚ö†Ô∏è Using CPU (no GPU/NPU available)")
    
    return device_info

def optimize_for_device(device_info: Dict[str, Any]) -> None:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –ë–ï–ó –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ô —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–∞ Replicate."""
    if device_info['type'] == 'cuda':
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã Replicate –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        total_memory_gb = device_info['memory']
        logger.info(f"üöÄ GPU detected ({total_memory_gb:.1f}GB) - using ALL available resources")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA –∫—ç—à–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        torch.backends.cudnn.benchmark = True  # –í–∫–ª—é—á–∏—Ç—å –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
        torch.backends.cuda.matmul.allow_tf32 = True  # –í–∫–ª—é—á–∏—Ç—å TF32 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        torch.backends.cudnn.allow_tf32 = True
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        torch.backends.cudnn.deterministic = False  # –û—Ç–∫–ª—é—á–∏—Ç—å –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        torch.backends.cudnn.enabled = True
        
        logger.info(f"üßπ CUDA cache cleared, MAXIMUM PERFORMANCE settings applied")
    
    elif device_info['type'] == 'npu':
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã NPU
        os.environ['INTEL_NPU_DEVICE'] = f"npu{device_info['id']}"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º 100% —Ä–µ—Å—É—Ä—Å–æ–≤ NPU
        max_cpu_percent = 100
        max_memory_percent = 100
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        os.environ['INTEL_NPU_MAX_CPU_USAGE'] = str(max_cpu_percent)
        os.environ['INTEL_NPU_MAX_MEMORY_USAGE'] = str(max_memory_percent)
        
        logger.info(f"üöÄ NPU optimizations enabled (max CPU: {max_cpu_percent}%, max memory: {max_memory_percent}%)")
    
    # –û–±—â–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    torch.set_num_threads(os.cpu_count())  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–¥—Ä–∞
    
    logger.info(f"‚úÖ Device optimization completed for {device_info['type']} ({device_info['name']}) - MAXIMUM PERFORMANCE")

def manage_gpu_memory(device_info: Dict[str, Any], operation: str = "check") -> None:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é GPU –ë–ï–ó –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ô –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ Replicate."""
    if device_info['type'] != 'cuda':
        return
        
    try:
        if operation == "clear":
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            torch.cuda.empty_cache()
            logger.info("üßπ GPU memory cache cleared for optimal performance")
            
        elif operation == "check":
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            allocated = torch.cuda.memory_allocated(device_info['id']) / (1024**3)
            reserved = torch.cuda.memory_reserved(device_info['id']) / (1024**3)
            total = device_info['memory']
            
            usage_percent = (allocated / total) * 100
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
            logger.info(f"üìä GPU memory usage: {usage_percent:.1f}% ({allocated:.1f}GB / {total:.1f}GB)")
                
    except Exception as e:
        logger.warning(f"GPU memory management error: {e}")

# Absolute cache paths inside the container
# Fixed paths for proper model loading in cog runtime
WEIGHTS_ROOT = "/src/model_files"
SDXL_CACHE_DIR = "/src/sdxl-cache"
CONTROLNET_CACHE_DIR = "/src/sdxl-cache"
REFS_DIR = "/src/model_files/refs"

# ControlNet local subfolders (pre-downloaded in build steps)
CONTROLNET_CANNY_DIR = os.path.join(CONTROLNET_CACHE_DIR, "controlnet-canny-sdxl-1.0")
# Soft-edge variant is sometimes distributed as HED for SDXL. Support both names.
CONTROLNET_HED_DIR = os.path.join(CONTROLNET_CACHE_DIR, "controlnet-hed-sdxl-1.0")
CONTROLNET_SOFTEDGE_DIR = os.path.join(CONTROLNET_CACHE_DIR, "controlnet-softedge-sdxl-1.0")
# Public alternative edge-like model available without gating
CONTROLNET_LINEART_DIR = os.path.join(CONTROLNET_CACHE_DIR, "controlnet-lineart-sdxl-1.0")

# HF repo ids for fallback when local cache is not baked into the image
SDXL_REPO_ID = "stabilityai/stable-diffusion-xl-base-1.0"
CONTROLNET_CANNY_REPO_ID = "diffusers/controlnet-canny-sdxl-1.0"
# –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–±–æ—á–∏–π repo ID –¥–ª—è Lineart ControlNet - SD 1.5 –≤–µ—Ä—Å–∏—è
CONTROLNET_LINEART_REPO_ID = "lllyasviel/control_v11p_sd15_scribble"


def hex_to_rgb(hex_color: str) -> Tuple[int, int, int]:
    h = hex_color.strip().lstrip("#")
    return tuple(int(h[i:i+2], 16) for i in (0, 2, 4))


def average_color_from_image(image_path: str) -> Tuple[int, int, int]:
    img = Image.open(image_path).convert("RGB")
    arr = np.asarray(img)
    mean = arr.reshape(-1, 3).mean(axis=0)
    return tuple(int(x) for x in mean.tolist())


def sample_color_for_name(color_name: str) -> Tuple[int, int, int]:
    color_folder = os.path.join(REFS_DIR, color_name)
    if os.path.isdir(color_folder):
        candidates: List[str] = []
        for fn in os.listdir(color_folder):
            if fn.lower().endswith((".png", ".jpg", ".jpeg")):
                candidates.append(os.path.join(color_folder, fn))
        if candidates:
            choice = random.choice(candidates)
            return average_color_from_image(choice)
    # Fallback simple mapping (extend as needed)
    named = {
        "black": (20, 20, 20),
        "white": (235, 235, 235),
        "red": (180, 40, 40),
        "green": (40, 160, 80),
        "blue": (40, 80, 180),
        "gray": (128, 128, 128),
        "brown": (120, 80, 50),
    }
    return named.get(color_name.lower(), (127, 127, 127))


def build_color_map(colors: List[Dict[str, Any]], size: Tuple[int, int], out_path: str) -> Image.Image:
    width, height = size
    canvas = Image.new("RGB", (width, height))
    draw = Image.new("RGB", (width, height))
    # Normalize proportions
    props = [max(0.0, float(c.get("proportion", 0))) for c in colors]
    total = sum(props) or 1.0
    props = [p / total for p in props]

    x0 = 0
    for c, p in zip(colors, props):
        w = int(round(p * width))
        if w <= 0:
            continue
        color = sample_color_for_name(c.get("name", "gray"))
        for x in range(x0, min(x0 + w, width)):
            for y in range(height):
                canvas.putpixel((x, y), color)
        x0 += w

    canvas.save(out_path)
    return canvas


def canny_edge_from_image(image: Image.Image, low_threshold: int, high_threshold: int) -> np.ndarray:
    # Convert PIL to OpenCV format
    img_array = np.array(image)
    if img_array.shape[2] == 3:  # RGB
        img_gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
    else:
        img_gray = img_array

    # Apply Canny edge detection
    edges = cv2.Canny(img_gray, low_threshold, high_threshold)
    
    # Convert back to PIL format
    return Image.fromarray(edges)


def select_controlnet_by_angle(angle: int, controlnet_canny: ControlNetModel, 
                              controlnet_softedge: ControlNetModel, 
                              controlnet_lineart: ControlNetModel) -> ControlNetModel:
    """Select ControlNet based on angle for optimal edge detection."""
    # Normalize angle to 0-360 range
    angle = angle % 360
    
    # For diagonal angles (30-60, 120-150, 210-240, 300-330), prefer Lineart
    if 30 <= angle <= 60 or 120 <= angle <= 150 or 210 <= angle <= 240 or 300 <= angle <= 330:
        if controlnet_lineart is not None:
            logger.info(f"Selected Lineart ControlNet for diagonal angle {angle}")
            return controlnet_lineart
        else:
            logger.warning("Lineart ControlNet not available, falling back to Canny")
    
    # For horizontal/vertical angles (0, 90, 180, 270), prefer Canny
    if angle in [0, 90, 180, 270]:
        if controlnet_canny is not None:
            logger.info(f"Selected Canny ControlNet for cardinal angle {angle}")
            return controlnet_canny
        else:
            logger.warning("Canny ControlNet not available, falling back to Softedge")
    
    # Default to Softedge for other angles
    if controlnet_softedge is not None:
        logger.info(f"Selected Softedge ControlNet for angle {angle}")
        return controlnet_softedge
    else:
        logger.warning("Softedge ControlNet not available, falling back to Canny")
        return controlnet_canny


class OptimizedPredictor(BasePredictor):
    def setup(self, weights: Optional[Path] = None) -> None:
        """Load the model into memory to make running multiple predictions efficient."""
        start_time = time.time()
        logger.info("Starting model setup...")
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –í—ã–±–æ—Ä –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª—É—á—à–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        self.device_info = select_best_device()
        optimize_for_device(self.device_info)
        
        # üöÄ –ù–û–í–û–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
        self.resource_monitor = ResourceMonitor(self.device_info)
        self.resource_monitor.start_monitoring()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è PyTorch
        if self.device_info['type'] == 'cuda':
            self.device = f"cuda:{self.device_info['id']}"
        elif self.device_info['type'] == 'npu':
            self.device = "cpu"  # NPU –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ diffusers
        else:
            self.device = "cpu"
        
        logger.info(f"üéØ Using device: {self.device} ({self.device_info['name']})")
        
        # üöÄ –ù–û–í–û–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é GPU
        manage_gpu_memory(self.device_info, "check")

        # üöÄ –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: Lazy Loading –¥–ª—è ControlNet –º–æ–¥–µ–ª–µ–π
        logger.info("üöÄ Initializing Lazy Loading Architecture for ControlNet models...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è Lazy Loading
        self.controlnet_canny = None
        self.controlnet_softedge = None
        self.controlnet_lineart = None
        self.controlnet_models_loaded = False
        
        # üöÄ –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
        if torch.cuda.is_available():
            available_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)
            free_memory = available_memory - allocated_memory
            
            logger.info(f"üìä Memory status before ControlNet loading:")
            logger.info(f"   Total: {available_memory:.1f}GB")
            logger.info(f"   Allocated: {allocated_memory:.1f}GB")
            logger.info(f"   Free: {free_memory:.1f}GB")
            
            # –ï—Å–ª–∏ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ –º–µ–Ω—å—à–µ 3GB, –æ—Ç–∫–ª–∞–¥—ã–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É ControlNet
            if free_memory < 3.0:
                logger.warning(f"‚ö†Ô∏è Insufficient memory ({free_memory:.1f}GB free), ControlNet will be loaded on-demand")
                self.controlnet_models_loaded = False
            else:
                logger.info(f"‚úÖ Sufficient memory available, proceeding with ControlNet pre-loading")
                self.controlnet_models_loaded = True
        else:
            self.controlnet_models_loaded = False
        
        # üöÄ Lazy Loading: ControlNet –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
        logger.info("üöÄ ControlNet models will be loaded on-demand to save memory")

        # üöÄ Lazy Loading: ControlNet –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
        # –≠—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏

        # üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SDXL —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é
        logger.info("üöÄ Initializing SDXL pipeline with memory management...")
        
        # üöÄ –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π SDXL
        if torch.cuda.is_available():
            available_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)
            free_memory = available_memory - allocated_memory
            
            logger.info(f"üìä Memory status before SDXL loading:")
            logger.info(f"   Total: {available_memory:.1f}GB")
            logger.info(f"   Allocated: {allocated_memory:.1f}GB")
            logger.info(f"   Free: {free_memory:.1f}GB")
            
            # –ï—Å–ª–∏ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ –º–µ–Ω—å—à–µ 4GB, –æ—á–∏—â–∞–µ–º –∫—ç—à
            if free_memory < 4.0:
                logger.warning(f"‚ö†Ô∏è Low memory ({free_memory:.1f}GB), clearing cache before SDXL loading")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        
        # üöÄ –°–¢–†–ê–¢–ï–ì–ò–Ø: –ù–∞—á–∏–Ω–∞–µ–º —Å –±–∞–∑–æ–≤–æ–≥–æ SDXL –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        try:
            logger.info("üöÄ Loading basic SDXL pipeline first...")
            from diffusers import StableDiffusionXLPipeline
            
            # üöÄ –ö–†–ò–¢–ò–ß–ù–û: –ó–∞–≥—Ä—É–∂–∞–µ–º SDXL –ë–ï–ó –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ô –ü–ê–ú–Ø–¢–ò –¥–ª—è Replicate
            self.pipe = StableDiffusionXLPipeline.from_pretrained(
                SDXL_REPO_ID,
                torch_dtype=torch.float16,
                use_safetensors=True,
                variant="fp16",
                safety_checker=None,
                requires_safety_checker=False,
                # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã Replicate
                low_cpu_mem_usage=True,
                device_map="auto" if torch.cuda.is_available() else None,
                # –£–ë–†–ê–ù–û: max_memory - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
            )
            
            # üöÄ –ö–†–ò–¢–ò–ß–ù–û: –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–∞–º—è—Ç–∏
            if torch.cuda.is_available():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ–º
                free_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3) - torch.cuda.memory_allocated(0) / (1024**3)
                if free_memory < 2.0:
                    logger.warning(f"‚ö†Ô∏è Very low memory ({free_memory:.1f}GB), using CPU fallback")
                    self.device = "cpu"
                    self.pipe = self.pipe.to("cpu")
                else:
                    self.pipe = self.pipe.to(self.device)
                    logger.info(f"‚úÖ SDXL pipeline moved to GPU ({self.device})")
            else:
                self.pipe = self.pipe.to(self.device)
            
            self.has_controlnet = False
            logger.info("‚úÖ Successfully initialized basic SDXL pipeline")
            
        except Exception as e:
            logger.error(f"‚ùå SDXL pipeline initialization failed: {e}")
            raise RuntimeError(f"Failed to initialize SDXL pipeline: {e}")
        
        if self.pipe is None:
            raise RuntimeError("Pipeline initialization failed completely")

        # Attach LoRA and Textual Inversion
        lora_path = "./model_files/rubber-tile-lora-v4_sdxl_lora.safetensors"
        ti_path = "./model_files/rubber-tile-lora-v4_sdxl_embeddings.safetensors"
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ LoRA —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        if os.path.exists(lora_path):
            try:
                logger.info("Loading LoRA weights...")
                self.pipe.load_lora_weights(lora_path)
                # Fuse for runtime speed
                self.pipe.fuse_lora()
                logger.info("‚úÖ LoRA weights loaded successfully")
            except Exception as e:
                logger.error(f"‚ùå Failed to load LoRA weights: {e}")
                raise RuntimeError(f"LoRA weights loading failed: {e}")
        else:
            logger.error(f"‚ùå LoRA weights not found at {lora_path}")
            raise RuntimeError(f"LoRA weights not found at {lora_path}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ Textual Inversion —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        if os.path.exists(ti_path):
            try:
                logger.info("Loading Textual Inversion embeddings...")
                # Try standard loader first (may fail for SDXL dual-encoder TI formats)
                try:
                    self.pipe.load_textual_inversion(ti_path, token="<s0>")
                    logger.info("‚úÖ Textual Inversion loaded with standard method")
                except Exception as e:
                    logger.warning(f"Standard TI load failed ({e}). Falling back to manual SDXL dual-encoder TI install...")
                    self._install_sdxl_textual_inversion_dual(ti_path, token_g="<s0>", token_l="<s0>")
                    logger.info("‚úÖ Textual Inversion loaded with manual method")
            except Exception as e:
                logger.error(f"‚ùå Failed to load Textual Inversion: {e}")
                raise RuntimeError(f"Textual Inversion loading failed: {e}")
        else:
            logger.error(f"‚ùå Textual inversion embeddings not found at {ti_path}")
            raise RuntimeError(f"Textual inversion embeddings not found at {ti_path}")

        # Scheduler —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            self.pipe.scheduler = EulerDiscreteScheduler.from_config(self.pipe.scheduler.config)
            logger.info("‚úÖ Scheduler configured successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Scheduler configuration failed: {e}")

        # Basic runtime opts —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            if hasattr(self.pipe, "enable_vae_slicing"):
                self.pipe.enable_vae_slicing()
                logger.info("‚úÖ VAE slicing enabled")
            if hasattr(self.pipe, "enable_vae_tiling"):
                self.pipe.enable_vae_tiling()
                logger.info("‚úÖ VAE tiling enabled")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è VAE optimization failed: {e}")
        
        # Performance optimizations - –æ—Ç–∫–ª—é—á–µ–Ω torch.compile –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å CUDA Graph
        # if hasattr(torch, 'compile') and torch.__version__ >= "2.4.0":
        #     try:
        #         logger.info("Enabling torch.compile for performance...")
        #         self.pipe = torch.compile(self.pipe, mode="reduce-overhead")
        #     except Exception as e:
        #         logger.warning(f"torch.compile failed: {e}")
        
        # CUDA optimizations —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            if self.device_info['type'] == 'cuda':
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –≤ optimize_for_device()
                logger.info("‚úÖ CUDA optimizations applied")
            elif self.device_info['type'] == 'npu':
                logger.info("‚úÖ NPU optimizations applied")
            else:
                logger.info("‚úÖ CPU optimizations applied")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Device optimization failed: {e}")

        setup_time = time.time() - start_time
        logger.info(f"üéâ Model setup completed successfully in {setup_time:.2f}s")
        logger.info(f"üìä ControlNet enabled: {self.has_controlnet}")
        logger.info(f"üîß Pipeline type: {type(self.pipe).__name__}")
        logger.info(f"üöÄ Device: {self.device} ({self.device_info['name']})")
        logger.info(f"üíæ Device memory: {self.device_info['memory']:.1f}GB")

    def _parse_params_json(self, params_json: str) -> Dict[str, Any]:
        """Clean parsing of params_json with proper error handling."""
        try:
            # First, try to parse the input directly
            if not params_json:
                return {}
            
            # Handle potential double-escaped JSON from web interface
            params = json.loads(params_json)
            
                                                # If params_json contains another params_json, extract it
            if "params_json" in params:
                inner_json = params["params_json"]
                if isinstance(inner_json, str):
                    params = json.loads(inner_json)
                else:
                    params = inner_json
            
            # Validate and clean the parsed parameters
            cleaned_params = {}
            
            # Colors validation
            if "colors" in params:
                colors = params["colors"]
                if isinstance(colors, list):
                    cleaned_colors = []
                    for color_info in colors:
                        if isinstance(color_info, dict):
                            name = color_info.get("name", "").strip()
                            proportion = color_info.get("proportion", 0)
                            
                            # Validate proportion (should be 0-100)
                            try:
                                proportion = float(proportion)
                                if 0 <= proportion <= 100:
                                    cleaned_colors.append({
                                        "name": name.lower(),
                                        "proportion": proportion
                                    })
                                else:
                                    logger.warning(f"Invalid proportion {proportion}, must be 0-100")
                            except (ValueError, TypeError):
                                logger.warning(f"Invalid proportion value: {proportion}")
                    
                    cleaned_params["colors"] = cleaned_colors
            
            # Other parameters
            for key in ["angle", "seed", "quality"]:
                if key in params:
                    value = params[key]
                    if key == "angle":
                        try:
                            cleaned_params[key] = int(value) % 360
                        except (ValueError, TypeError):
                            cleaned_params[key] = 0
                    elif key == "seed":
                        try:
                            cleaned_params[key] = int(value)
                        except (ValueError, TypeError):
                            cleaned_params[key] = -1
                    elif key == "quality":
                        if value in ["preview", "standard", "high"]:
                            cleaned_params[key] = value
                        else:
                            cleaned_params[key] = "standard"
            
            # Overrides validation
            if "overrides" in params and isinstance(params["overrides"], dict):
                overrides = params["overrides"]
                cleaned_overrides = {}
                
                # ControlNet setting
                if "use_controlnet" in overrides:
                    cleaned_overrides["use_controlnet"] = bool(overrides["use_controlnet"])
                
                # Guidance scale
                if "guidance_scale" in overrides:
                    try:
                        guidance = float(overrides["guidance_scale"])
                        if 1.0 <= guidance <= 20.0:
                            cleaned_overrides["guidance_scale"] = guidance
                        else:
                            logger.warning(f"Invalid guidance_scale {guidance}, using default")
                    except (ValueError, TypeError):
                        logger.warning(f"Invalid guidance_scale value: {overrides['guidance_scale']}")
                
                # Steps overrides
                for key in ["num_inference_steps_preview", "num_inference_steps_final"]:
                    if key in overrides:
                        try:
                            steps = int(overrides[key])
                            if 1 <= steps <= 100:
                                cleaned_overrides[key] = steps
                            else:
                                logger.warning(f"Invalid {key} {steps}, using default")
                        except (ValueError, TypeError):
                            logger.warning(f"Invalid {key} value: {overrides[key]}")
                
                if cleaned_overrides:
                    cleaned_params["overrides"] = cleaned_overrides
            
            logger.info(f"Parsed parameters: {cleaned_params}")
            return cleaned_params
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise ValueError(f"Invalid JSON format: {e}")
        except Exception as e:
            logger.error(f"Unexpected error parsing params: {e}")
            raise ValueError(f"Failed to parse parameters: {e}")

    def _build_prompt(self, colors: List[Dict[str, Any]]) -> str:
        """Build clean prompt from color information."""
        prompt_parts = ["ohwx_rubber_tile <s0><s1>"]
        
        # Add color proportions to prompt
        if colors:
            color_desc = []
            for color_info in colors:
                name = color_info.get("name", "").lower()
                proportion = color_info.get("proportion", 0)
                if proportion > 0:
                    # proportion —É–∂–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (0-100), –Ω–µ —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100
                    percentage = int(proportion)
                    color_desc.append(f"{percentage}% {name}")
            
            if color_desc:
                prompt_parts.append(", ".join(color_desc))
        
        # Add quality descriptors
        prompt_parts.extend([
            "photorealistic rubber tile",
            "matte texture", 
            "top view",
            "rubber granules",
            "textured surface"
        ])
        
        return ", ".join(prompt_parts)

    def _should_use_controlnet(self, angle: int) -> Tuple[bool, str]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ControlNet –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —É–≥–ª–∞.
        
        Args:
            angle: –£–≥–æ–ª —É–∫–ª–∞–¥–∫–∏ –≤ –≥—Ä–∞–¥—É—Å–∞—Ö
            
        Returns:
            Tuple[bool, str]: (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å_controlnet, –ø—Ä–∏—á–∏–Ω–∞)
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —É–≥–ª–∞
        angle_norm = int(angle) % 180
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–≥–æ–ª 0¬∞ - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –±–µ–∑ ControlNet
        if angle_norm == 0:
            return False, "–£–≥–æ–ª 0¬∞ (–≤–∏–¥ —Å–≤–µ—Ä—Ö—É) - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞–∫—É—Ä—Å –±–µ–∑ ControlNet"
        
        # –î–ª—è –¥—Ä—É–≥–∏—Ö —É–≥–ª–æ–≤ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ControlNet —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
        if angle_norm in (45, 135):
            return True, f"–£–≥–æ–ª {angle}¬∞ —Ç—Ä–µ–±—É–µ—Ç ControlNet –¥–ª—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è"
        elif angle_norm in (30, 60, 90, 120, 150):
            return True, f"–£–≥–æ–ª {angle}¬∞ —Ç—Ä–µ–±—É–µ—Ç ControlNet –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
        else:
            return True, f"–£–≥–æ–ª {angle}¬∞ —Ç—Ä–µ–±—É–µ—Ç ControlNet (–Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∫—É—Ä—Å)"

    def _load_controlnet_models_on_demand(self) -> None:
        """
        üöÄ Lazy Loading: –ó–∞–≥—Ä—É–∂–∞–µ—Ç ControlNet –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
        """
        if self.controlnet_models_loaded:
            return
            
        logger.info("üöÄ Loading ControlNet models on-demand...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
        if torch.cuda.is_available():
            available_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)
            free_memory = available_memory - allocated_memory
            
            logger.info(f"üìä Memory status before ControlNet loading:")
            logger.info(f"   Total: {available_memory:.1f}GB")
            logger.info(f"   Allocated: {allocated_memory:.1f}GB")
            logger.info(f"   Free: {free_memory:.1f}GB")
            
            # –ï—Å–ª–∏ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ –º–µ–Ω—å—à–µ 2GB, –æ—á–∏—â–∞–µ–º –∫—ç—à
            if free_memory < 2.0:
                logger.warning(f"‚ö†Ô∏è Low memory ({free_memory:.1f}GB), clearing cache before loading")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ Canny ControlNet
            if os.path.exists(CONTROLNET_CANNY_DIR):
                self.controlnet_canny = ControlNetModel.from_pretrained(CONTROLNET_CANNY_DIR)
                logger.info("‚úÖ Canny ControlNet loaded from local cache")
            else:
                logger.info("Canny ControlNet not found in local cache, will download from HF")
                
            # –ó–∞–≥—Ä—É–∑–∫–∞ Softedge ControlNet
            if os.path.exists(CONTROLNET_SOFTEDGE_DIR):
                self.controlnet_softedge = ControlNetModel.from_pretrained(CONTROLNET_SOFTEDGE_DIR)
                logger.info("‚úÖ Softedge ControlNet loaded from local cache")
            else:
                logger.info("Softedge ControlNet not found in local cache, will download from HF")
                
            # –ó–∞–≥—Ä—É–∑–∫–∞ Lineart ControlNet
            if os.path.exists(CONTROLNET_LINEART_DIR):
                self.controlnet_lineart = ControlNetModel.from_pretrained(CONTROLNET_LINEART_DIR)
                logger.info("‚úÖ Lineart ControlNet loaded from local cache")
            else:
                logger.info("Lineart ControlNet not found in local cache, will download from HF")
                
            self.controlnet_models_loaded = True
            logger.info("‚úÖ All ControlNet models loaded successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load ControlNet models: {e}")
            self.controlnet_models_loaded = False

    def _get_controlnet_model(self, angle: int) -> Optional[Any]:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â—É—é ControlNet –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —É–≥–ª–∞.
        
        Args:
            angle: –£–≥–æ–ª —É–∫–ª–∞–¥–∫–∏ –≤ –≥—Ä–∞–¥—É—Å–∞—Ö
            
        Returns:
            ControlNet –º–æ–¥–µ–ª—å –∏–ª–∏ None
        """
        # üöÄ Lazy Loading: –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if not self.controlnet_models_loaded:
            self._load_controlnet_models_on_demand()
        
        angle_norm = int(angle) % 180
        
        # –î–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã—Ö/–≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã—Ö —É–≥–ª–æ–≤ - Canny
        if angle_norm in (0, 90):
            if self.controlnet_canny:
                return self.controlnet_canny
            elif self.controlnet_softedge:
                return self.controlnet_softedge
        
        # –î–ª—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã—Ö —É–≥–ª–æ–≤ - Lineart –∏–ª–∏ SoftEdge
        elif angle_norm in (30, 45, 60, 120, 135, 150):
            if self.controlnet_lineart:
                return self.controlnet_lineart
            elif self.controlnet_softedge:
                return self.controlnet_softedge
            elif self.controlnet_canny:
                return self.controlnet_canny
        
        # Fallback –Ω–∞ –ª—é–±—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
        for model in [self.controlnet_canny, self.controlnet_softedge, self.controlnet_lineart]:
            if model:
                return model
        
        return None

    def predict(
        self,
        params_json: str = Input(description="Business-oriented parameters JSON: colors, angle, seed, quality, overrides. –í–ê–ñ–ù–û: –£–≥–æ–ª 0¬∞ - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞–∫—É—Ä—Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –î—Ä—É–≥–∏–µ —É–≥–ª—ã —Ç—Ä–µ–±—É—é—Ç ControlNet –¥–ª—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è.")
    ) -> List[Path]:
        """Generate preview/final images using ControlNet color‚Äëcomposition guidance.

        Returns: [preview.png, final.png, colormap.png]
        """
        start_time = time.time()
        
        # üöÄ –ù–û–í–û–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        logger.info("üîç Checking device resources before generation...")
        manage_gpu_memory(self.device_info, "check")
        
        # Parse and validate input parameters
        try:
            params = self._parse_params_json(params_json)
        except Exception as e:
            raise ValueError(f"Parameter validation failed: {e}")

        colors = params.get("colors", [])
        angle = int(params.get("angle", 0))
        seed = int(params.get("seed", -1))
        quality = str(params.get("quality", "standard"))
        overrides: Dict[str, Any] = params.get("overrides", {}) or {}

        logger.info(f"Generating with params: colors={len(colors)}, angle={angle}, quality={quality}, seed={seed}")
        
        # üöÄ –ù–û–í–û–ï: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
        if hasattr(self, 'resource_monitor'):
            resource_summary = self.resource_monitor.get_resource_summary()
            logger.info(f"üìä Resource status: {resource_summary}")

        # üöÄ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ü–†–û–§–ò–õ–ò –ö–ê–ß–ï–°–¢–í–ê: –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if quality == "preview":
            steps_preview, steps_final = 35, 40  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            size_preview, size_final = (512, 512), (1024, 1024)
            guidance_scale_default = 6.5  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –º–µ–Ω—å—à–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        elif quality == "high":
            steps_preview, steps_final = 40, 60  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ø—Ä–µ–º–∏—É–º –∫–∞—á–µ—Å—Ç–≤–∞
            size_preview, size_final = (512, 512), (1024, 1024)
            guidance_scale_default = 6.0  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        else:  # standard
            steps_preview, steps_final = 35, 50  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 24,40 –¥–æ 35,50
            size_preview, size_final = (512, 512), (1024, 1024)
            guidance_scale_default = 6.5  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–Ω–∏–∂–µ–Ω–æ —Å 7.5 –¥–æ 6.5

        # Apply overrides
        num_inference_steps_preview = int(overrides.get("num_inference_steps_preview", steps_preview))
        num_inference_steps_final = int(overrides.get("num_inference_steps_final", steps_final))
        guidance_scale = float(overrides.get("guidance_scale", guidance_scale_default))  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

        # Build clean prompt
        base_prompt = self._build_prompt(colors)
        logger.info(f"Generated prompt: {base_prompt}")

        negative_prompt = overrides.get(
            "negative_prompt",
            "object, blurry, worst quality, low quality, deformed, watermark, 3d render, cartoon, abstract, smooth, flat",
        )

        # Generator
        generator = torch.manual_seed(seed) if seed != -1 else torch.Generator(device=self.device)
        if seed == -1:
            seed = generator.seed()

        # Create color map and corresponding control image (edge map)
        colormap_path = "/tmp/colormap.png"
        logger.info(f"Building color map for {len(colors)} colors")
        colormap_img = build_color_map(colors, size_final, colormap_path)
        logger.info(f"Color map saved to {colormap_path}")

        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –õ–æ–≥–∏–∫–∞ —É–≥–ª–æ–≤ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏
        should_use_controlnet, reason = self._should_use_controlnet(angle)
        use_controlnet_by_angle = should_use_controlnet
        
        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π override ControlNet
        user_controlnet_setting = overrides.get("use_controlnet", None)
        
        if user_controlnet_setting is not None:
            use_controlnet = bool(user_controlnet_setting)
            if use_controlnet != use_controlnet_by_angle:
                logger.warning(f"‚ö†Ô∏è –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏–ª ControlNet: {use_controlnet} (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: {use_controlnet_by_angle})")
                logger.warning(f"‚ö†Ô∏è –ü—Ä–∏—á–∏–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {reason}")
        else:
            use_controlnet = use_controlnet_by_angle
            
        logger.info(f"ControlNet —Ä–µ—à–µ–Ω–∏–µ: {use_controlnet} - {reason}")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å ControlNet
        control_preview = None
        control_final = None
        
        if use_controlnet and self.has_controlnet:
            try:
        # Select controlnet by angle
                selected_cn = select_controlnet_by_angle(
                    angle, self.controlnet_canny, self.controlnet_softedge, self.controlnet_lineart
                )
                if selected_cn is not None:
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ControlNet
                    if hasattr(self.pipe, 'controlnet'):
                        self.pipe.controlnet = selected_cn
                        logger.info(f"‚úÖ ControlNet set for angle {angle}")

                        # Prepare edge maps for preview/final
                        logger.info("Generating edge maps...")
                        control_preview = canny_edge_from_image(colormap_img.resize(size_preview, Image.NEAREST), 80, 160)
                        control_final = canny_edge_from_image(colormap_img.resize(size_final, Image.NEAREST), 100, 200)
                        logger.info("‚úÖ Edge maps generated successfully")
                    else:
                        logger.warning("‚ö†Ô∏è Pipeline does not support ControlNet")
                        use_controlnet = False
                else:
                    logger.warning("‚ö†Ô∏è No ControlNet available for this angle")
                    use_controlnet = False
            except Exception as e:
                logger.error(f"‚ùå ControlNet setup failed: {e}")
                use_controlnet = False
                control_preview = None
                control_final = None
        else:
            logger.info("‚ÑπÔ∏è ControlNet disabled (user preference or not available)")

        # Generate preview first (fast)
        preview_start = time.time()
        logger.info(f"Generating preview with {num_inference_steps_preview} steps, guidance_scale={guidance_scale}")
        logger.info(f"ControlNet status: enabled={use_controlnet}, available={self.has_controlnet}, image={control_preview is not None}")
        
        try:
            # Prepare generation parameters
            gen_params = {
                "prompt": base_prompt,
                "negative_prompt": negative_prompt,
                "width": size_preview[0],
                "height": size_preview[1],
                "num_inference_steps": num_inference_steps_preview,
                "guidance_scale": guidance_scale,
                "generator": generator,
            }
            
            # Add ControlNet image if enabled and available
            if use_controlnet and control_preview is not None and self.has_controlnet:
                gen_params["image"] = control_preview
                logger.info("‚úÖ Using ControlNet for preview generation")
            else:
                logger.info("‚ÑπÔ∏è Preview generation without ControlNet")
            
            # üßπ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï v4.3.10: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π GPU
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                torch.cuda.reset_peak_memory_stats()  # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞–º—è—Ç–∏
                gc.collect()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
                torch.cuda.empty_cache()  # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                
                # üöÄ –ù–û–í–û–ï: –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏
                logger.info("üîß –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")
                
                logger.info("üßπ –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã")
            
            # üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï v4.3.10: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π VAE decode —Å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é —Ç–∏–ø–æ–≤
            # üöÄ –ù–û–í–û–ï: –û—Å—Ç–∞–≤–ª—è–µ–º –í–ï–°–¨ pipeline –Ω–∞ GPU –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            logger.info("üîß Final generation —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º final —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU
            with torch.no_grad():
                final = self.pipe(**gen_params).images[0]
            
            logger.info("üîß Final generation –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            # üöÄ –ù–û–í–û–ï: –û—Å—Ç–∞–≤–ª—è–µ–º –í–ï–°–¨ pipeline –Ω–∞ GPU –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            logger.info("üîß Preview generation —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º preview —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU
            with torch.no_grad():
                preview = self.pipe(**gen_params).images[0]
            
            logger.info("üîß Preview generation –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            try:
                
                preview_time = time.time() - preview_start
                logger.info(f"‚úÖ Preview generated successfully in {preview_time:.2f}s")
                
            except Exception as e:
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º VAE –Ω–∞ GPU
                if hasattr(self.pipe, 'vae') and hasattr(self.pipe.vae, 'to'):
                    self.pipe.vae = self.pipe.vae.to(original_vae_device)
                logger.error(f"‚ùå Preview generation error: {e}")
                raise e
        except Exception as e:
            logger.error(f"‚ùå Preview generation failed: {e}")
            raise RuntimeError(f"Preview generation failed: {e}")

        # Generate final (quality)
        final_start = time.time()
        logger.info(f"Generating final image with {num_inference_steps_final} steps")
        
        try:
            # Prepare generation parameters
            gen_params = {
                "prompt": base_prompt,
                "negative_prompt": negative_prompt,
                "width": size_final[0],
                "height": size_final[1],
                "num_inference_steps": num_inference_steps_final,
                "guidance_scale": guidance_scale,
                "generator": generator,
            }
            
            # Add ControlNet image if enabled and available
            if use_controlnet and control_final is not None and self.has_controlnet:
                gen_params["image"] = control_final
                logger.info("‚úÖ Using ControlNet for final generation")
            else:
                logger.info("‚ÑπÔ∏è Final generation without ControlNet")
            
            # üßπ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï v4.3.10: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π GPU
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                torch.cuda.reset_peak_memory_stats()  # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞–º—è—Ç–∏
                gc.collect()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
                torch.cuda.empty_cache()  # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                
                # üöÄ –ù–û–í–û–ï: –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏
                logger.info("üîß –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")
                
                logger.info("üßπ –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã")
            
            # üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï v4.3.10: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π VAE decode —Å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é —Ç–∏–ø–æ–≤
            # üöÄ –ù–û–í–û–ï: –û—Å—Ç–∞–≤–ª—è–µ–º –í–ï–°–¨ pipeline –Ω–∞ GPU –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            logger.info("üîß Final generation —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º final —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU
            with torch.no_grad():
                final = self.pipe(**gen_params).images[0]
            
            logger.info("üîß Final generation –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            # üöÄ –ù–û–í–û–ï: –û—Å—Ç–∞–≤–ª—è–µ–º –í–ï–°–¨ pipeline –Ω–∞ GPU –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            logger.info("üîß Preview generation —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º preview —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU
            with torch.no_grad():
                preview = self.pipe(**gen_params).images[0]
            
            logger.info("üîß Preview generation –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            try:
