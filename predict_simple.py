# predict_simple.py - –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –∏ GPU/NPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
import os
import json
import time
import logging
import warnings
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

import torch
import numpy as np
from PIL import Image
import cv2

from diffusers import (
    StableDiffusionXLPipeline,
    ControlNetModel,
    DPMSolverMultistepScheduler,
)
from transformers import CLIPTextModel, CLIPTokenizer
from peft import PeftModel

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –£–ë–†–ê–ù–û: ResourceMonitor –∫–ª–∞—Å—Å - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

# –£–ë–†–ê–ù–û: –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –î–õ–Ø MULTI-GPU –ò NPU - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

# –£–ë–†–ê–ù–û: –ò–°–ü–†–ê–í–õ–ï–ù–û v4.3.15: ACCELERATE –ü–û–õ–ù–û–°–¢–¨–Æ –£–î–ê–õ–ï–ù - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

# –£–ë–†–ê–ù–û: –§–∏–ª—å—Ç—Ä—ã –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

class OptimizedPredictor:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∑–∏–Ω–æ–≤—ã—Ö –ø–ª–∏—Ç–æ–∫."""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.pipe = None
        self.has_controlnet = False
        self.controlnet_canny = None
        self.controlnet_softedge = None
        self.controlnet_lineart = None
        
    def setup(self, weights: Optional[str] = None) -> None:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏."""
        logger.info("Starting model setup...")
        
        # –ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if torch.cuda.is_available():
            logger.info(f"Found {torch.cuda.device_count()} CUDA GPU(s)")
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                logger.info(f"GPU {i}: {props.name} - {props.total_memory / (1024**3):.1f}GB")
            logger.info(f"‚úÖ Selected GPU 0: {torch.cuda.get_device_properties(0).name} ({torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB)")
            logger.info("üöÄ GPU detected - using CUDA")
        else:
            logger.info("‚ö†Ô∏è No CUDA GPU found - using CPU")
            self.device = "cpu"
        
        logger.info(f"üéØ Using device: {self.device}")
        
        # –£–ë–†–ê–ù–û: Resource monitoring - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        
        # –£–ë–†–ê–ù–û: Lazy Loading Architecture - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        
        # –£–ë–†–ê–ù–û: Memory management - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SDXL pipeline
        logger.info("üöÄ Initializing SDXL pipeline...")
        
        try:
            self.pipe = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch.float16,
                use_safetensors=True,
                variant="fp16",
            )
            
            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.pipe = self.pipe.to(self.device)
            logger.info(f"‚úÖ SDXL pipeline moved to {self.device}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ LoRA
            logger.info("Loading LoRA weights...")
            self.pipe.load_lora_weights("nauslava/plitka-pro-lora-v4")
            logger.info("‚úÖ LoRA weights loaded successfully")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ Textual Inversion
            logger.info("Loading Textual Inversion embeddings...")
            try:
                self.pipe.load_textual_inversion("nauslava/plitka-pro-textual-inversion-v4")
                logger.info("‚úÖ Textual Inversion loaded successfully")
            except Exception as e:
                logger.warning(f"Standard TI load failed ({e}). Falling back to manual SDXL dual-encoder TI install...")
                self._install_sdxl_textual_inversion_dual("nauslava/plitka-pro-textual-inversion-v4", self.pipe, token_g="<s0>", token_l="<s0>")
                logger.info("‚úÖ Textual Inversion loaded with manual method")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ scheduler
            self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)
            logger.info("‚úÖ Scheduler configured successfully")
            
            # VAE –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            self.pipe.vae.enable_slicing()
            self.pipe.vae.enable_tiling()
            logger.info("‚úÖ VAE slicing enabled")
            logger.info("‚úÖ VAE tiling enabled")
            
            logger.info("üéâ Model setup completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Model setup failed: {e}")
            raise e
    
    def _install_sdxl_textual_inversion_dual(self, ti_path: str, pipeline, token_g: str, token_l: str) -> None:
        """Install SDXL textual inversion that contains separate embeddings for CLIP-G and CLIP-L encoders."""
        logger.info("üî§ Installing dual-encoder SDXL textual inversion...")
        
        try:
            # Load the textual inversion embeddings
            from safetensors import safe_open
            
            # Load embeddings from the model path
            model_path = ti_path
            if not os.path.exists(model_path):
                # Try to download from Hugging Face Hub
                from huggingface_hub import snapshot_download
                model_path = snapshot_download(repo_id=ti_path)
            
            # Load the embeddings
            with safe_open(os.path.join(model_path, "learned_embeds.safetensors"), framework="pt", device="cpu") as f:
                string_to_param = {key: value for key, value in f.get_tensor("string_to_param").items()}
            
            # Extract token names
            token_names = list(string_to_param.keys())
            logger.info(f"üî§ Textual inversion contains {len(token_names)} token(s)")
            logger.info(f"üî§ Token names: {token_names}")
            
            # Install embeddings for both CLIP-G and CLIP-L
            pipeline.load_textual_inversion(model_path, weight_name="learned_embeds.safetensors")
            
            logger.info(f"‚úÖ SDXL textual inversion installed manually for {len(token_names)} token(s): {token_names}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to install SDXL textual inversion: {e}")
            raise e
    
    def _build_prompt(self, colors: List[Dict[str, Any]]) -> str:
        """Build prompt from color specifications."""
        if not colors:
            return "ohwx_rubber_tile <s0><s1>, 100% white"
        
        color_parts = []
        for color_info in colors:
            name = color_info.get("name", "white")
            proportion = color_info.get("proportion", 100)
            color_parts.append(f"{proportion}% {name}")
        
        color_str = ", ".join(color_parts)
        return f"ohwx_rubber_tile <s0><s1>, {color_str}, photorealistic rubber tile, matte texture, top view, rubber granules, textured surface"
    
    def _should_use_controlnet(self, angle: int) -> Tuple[bool, str]:
        """Determine if ControlNet should be used based on angle."""
        if angle == 0:
            return False, "–£–≥–æ–ª 0¬∞ (–≤–∏–¥ —Å–≤–µ—Ä—Ö—É) - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Ä–∞–∫—É—Ä—Å –±–µ–∑ ControlNet"
        else:
            return True, f"–£–≥–æ–ª {angle}¬∞ —Ç—Ä–µ–±—É–µ—Ç ControlNet –¥–ª—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è"
    
    def predict(self, params_json: str) -> List[str]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."""
        start_time = time.time()
        
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            params = json.loads(params_json)
            
            colors = params.get("colors", [])
            angle = int(params.get("angle", 0))
            seed = int(params.get("seed", -1))
            quality = str(params.get("quality", "standard"))
            overrides: Dict[str, Any] = params.get("overrides", {}) or {}
            
            logger.info(f"Generating with params: colors={len(colors)}, angle={angle}, quality={quality}, seed={seed}")
            
            # –£–ë–†–ê–ù–û: Resource status logging - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            
            # –£–ë–†–ê–ù–û: –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ü–†–û–§–ò–õ–ò –ö–ê–ß–ï–°–¢–í–ê v4.3.15 - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞
            if quality == "preview":
                steps_preview, steps_final = 25, 30
                size_preview, size_final = (512, 512), (1024, 1024)
                guidance_scale_default = 5.5
            elif quality == "high":
                steps_preview, steps_final = 50, 80
                size_preview, size_final = (512, 512), (1024, 1024)
                guidance_scale_default = 5.0
            else:  # standard
                steps_preview, steps_final = 40, 60
                size_preview, size_final = (512, 512), (1024, 1024)
                guidance_scale_default = 5.5
            
            # Apply overrides
            num_inference_steps_preview = int(overrides.get("num_inference_steps_preview", steps_preview))
            num_inference_steps_final = int(overrides.get("num_inference_steps_final", steps_final))
            guidance_scale = float(overrides.get("guidance_scale", guidance_scale_default))
            
            # Build clean prompt
            base_prompt = self._build_prompt(colors)
            logger.info(f"Generated prompt: {base_prompt}")
            
            negative_prompt = overrides.get(
                "negative_prompt",
                "object, blurry, worst quality, low quality, deformed, watermark, 3d render, cartoon, abstract, smooth, flat",
            )
            
            # Generator
            generator = torch.manual_seed(seed) if seed != -1 else torch.Generator(device=self.device)
            if seed == -1:
                seed = generator.seed()
            
            # Create color map
            colormap_path = "/tmp/colormap.png"
            logger.info(f"Building color map for {len(colors)} colors")
            colormap_img = self._build_color_map(colors, size_final, colormap_path)
            logger.info(f"Color map saved to {colormap_path}")
            
            # –£–ë–†–ê–ù–û: ControlNet logic - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            use_controlnet = False
            control_preview = None
            control_final = None
            
            # Generate preview
            preview_start = time.time()
            logger.info(f"Generating preview with {num_inference_steps_preview} steps, guidance_scale={guidance_scale}")
            
            # –£–ë–†–ê–ù–û: ControlNet status logging - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            
            # –£–ë–†–ê–ù–û: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º preview —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ preview
            preview_params = {
                "prompt": base_prompt,
                "negative_prompt": negative_prompt,
                "width": size_preview[0],
                "height": size_preview[1],
                "num_inference_steps": num_inference_steps_preview,
                "guidance_scale": guidance_scale,
                "generator": generator,
            }
            
            # Add ControlNet image if enabled and available
            if use_controlnet and control_preview is not None and self.has_controlnet:
                preview_params["image"] = control_preview
                logger.info("‚úÖ Using ControlNet for preview generation")
            
            logger.info("üîß Preview generation —Å –ø–æ–ª–Ω—ã–º pipeline –Ω–∞ GPU")
            with torch.no_grad():
                preview = self.pipe(**preview_params).images[0]
            
            logger.info("üîß Preview generation –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # Generate final
            final_start = time.time()
            logger.info(f"Generating final image with {num_inference_steps_final} steps")
            
            # Prepare generation parameters
            gen_params = {
                "prompt": base_prompt,
                "negative_prompt": negative_prompt,
                "width": size_final[0],
                "height": size_final[1],
                "num_inference_steps": num_inference_steps_final,
                "guidance_scale": guidance_scale,
                "generator": generator,
            }
            
            # Add ControlNet image if enabled and available
            if use_controlnet and control_final is not None and self.has_controlnet:
                gen_params["image"] = control_final
                logger.info("‚úÖ Using ControlNet for final generation")
            else:
                logger.info("‚ÑπÔ∏è Final generation without ControlNet")
            
            # –£–ë–†–ê–ù–û: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é - —É–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º final —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ final
            with torch.no_grad():
                final = self.pipe(**gen_params).images[0]
            
            logger.info("üîß Final generation –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            preview_path = Path("/tmp/preview.png")
            final_path = Path("/tmp/final.png")
            colormap_path = Path("/tmp/colormap.png")
            
            preview.save(preview_path)
            final.save(final_path)
            colormap_img.save(colormap_path)
            
            total_time = time.time() - start_time
            logger.info(f"‚úÖ Generation completed in {total_time:.2f}s")
            
            return [str(preview_path), str(final_path), str(colormap_path)]
            
        except Exception as e:
            logger.error(f"‚ùå Generation failed: {e}")
            raise RuntimeError(f"Generation failed: {e}")
    
    def _build_color_map(self, colors: List[Dict[str, Any]], size: Tuple[int, int], output_path: str) -> Image.Image:
        """Build a simple color map image."""
        width, height = size
        
        # Create a simple color map
        if not colors:
            # Default white
            color_map = Image.new('RGB', (width, height), (255, 255, 255))
        else:
            # Create gradient based on colors
            color_map = Image.new('RGB', (width, height))
            pixels = color_map.load()
            
            for y in range(height):
                for x in range(width):
                    # Simple color mixing based on proportions
                    r, g, b = 0, 0, 0
                    total_proportion = sum(color.get("proportion", 0) for color in colors)
                    
                    for color_info in colors:
                        proportion = color_info.get("proportion", 0) / total_proportion
                        color_name = color_info.get("name", "white").lower()
                        
                        # Simple color mapping
                        if color_name == "red":
                            r += int(255 * proportion)
                        elif color_name == "green":
                            g += int(255 * proportion)
                        elif color_name == "blue":
                            b += int(255 * proportion)
                        elif color_name == "yellow":
                            r += int(255 * proportion)
                            g += int(255 * proportion)
                        elif color_name == "purple":
                            r += int(128 * proportion)
                            b += int(255 * proportion)
                        elif color_name == "orange":
                            r += int(255 * proportion)
                            g += int(128 * proportion)
                        elif color_name == "black":
                            pass  # Keep black
                        else:  # white or unknown
                            r += int(255 * proportion)
                            g += int(255 * proportion)
                            b += int(255 * proportion)
                    
                    pixels[x, y] = (min(255, r), min(255, g), min(255, b))
        
        return color_map
