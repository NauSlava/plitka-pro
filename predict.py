# –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –î–û–õ–ñ–ù–´ –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –î–û –∏–º–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
import os
# –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º HF_HOME –≤–º–µ—Å—Ç–æ TRANSFORMERS_CACHE (deprecated)
os.environ["HF_HOME"] = "/tmp/hf_home"
os.environ["HF_DATASETS_CACHE"] = "/tmp/hf_datasets_cache"
os.environ["HF_MODELS_CACHE"] = "/tmp/hf_models_cache"
# –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª—é—á–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏—é –∫—ç—à–∞ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
os.environ["TRANSFORMERS_CACHE_MIGRATION_DISABLE"] = "1"
os.environ["HF_HUB_CACHE_MIGRATION_DISABLE"] = "1"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"  # –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏, –±–µ–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
# –ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º OFFLINE —Ä–µ–∂–∏–º - —ç—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π

import json
import logging
import warnings
import gc
import torch
import random
from pathlib import Path
from typing import List, Dict, Any
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import cv2 # Added for Canny edge detection

from cog import BasePredictor, Input
from diffusers import (
    StableDiffusionXLControlNetPipeline,
    StableDiffusionXLPipeline,
    DPMSolverMultistepScheduler,
    ControlNetModel
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
os.environ["TRANSFORMERS_VERBOSITY"] = "warning"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
# –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
# –ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º OFFLINE —Ä–µ–∂–∏–º - —ç—Ç–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "500"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π diffusers –±–µ–∑ –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è
def handle_diffusers_warnings():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è diffusers –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º."""
    import warnings
    import torch.utils._pytree as _pytree
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∑–∞–º–µ–Ω–∞ deprecated –º–µ—Ç–æ–¥–∞
    if hasattr(_pytree, "_register_pytree_node"):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
        _pytree._register_pytree_node_fixed = _pytree._register_pytree_node
        # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
        _pytree._register_pytree_node = _pytree.register_pytree_node
        logger.info("‚úÖ Pytree warnings –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã —á–µ—Ä–µ–∑ monkey patching")
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
    def custom_pytree_warning_filter(message, category, filename, lineno, file=None, line=None):
        message_str = str(message)
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        if "torch.utils._pytree._register_pytree_node" in message_str:
            # –≠—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ deprecated API - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–±
            return True  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ diffusers
        elif "TRANSFORMERS_CACHE" in message_str:
            # –≠—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ deprecated TRANSFORMERS_CACHE
            return True  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –º—ã —É–∂–µ –∏—Å–ø—Ä–∞–≤–∏–ª–∏ —ç—Ç–æ
        elif "cache for model files in Transformers" in message_str:
            # –≠—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –º–∏–≥—Ä–∞—Ü–∏–∏ –∫—ç—à–∞
            return True  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –º—ã –æ—Ç–∫–ª—é—á–∏–ª–∏ –º–∏–≥—Ä–∞—Ü–∏—é
        elif "Migrating your old cache" in message_str:
            # –≠—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –º–∏–≥—Ä–∞—Ü–∏–∏ –∫—ç—à–∞
            return True  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –º—ã –æ—Ç–∫–ª—é—á–∏–ª–∏ –º–∏–≥—Ä–∞—Ü–∏—é
        elif "You are offline and the cache" in message_str:
            # –≠—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –æ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º–µ
            return True  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –º—ã –∏—Å–ø—Ä–∞–≤–∏–ª–∏ —ç—Ç–æ
        elif "multivariate normal distribution" in message_str:
            # –≠—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ embeddings
            return True  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –º—ã –∏—Å–ø—Ä–∞–≤–∏–ª–∏ —ç—Ç–æ
        elif "The new embeddings will be initialized" in message_str:
            # –≠—Ç–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ embeddings
            return True  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –º—ã –∏—Å–ø—Ä–∞–≤–∏–ª–∏ —ç—Ç–æ
        return False  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
    
    warnings.showwarning = custom_pytree_warning_filter

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
handle_diffusers_warnings()



class Predictor(BasePredictor):
    def setup(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞."""
        logger.info("Starting model setup...")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info(f"Found {gpu_count} CUDA GPU(s)")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                logger.info(f"GPU {i}: {gpu_name} - {gpu_memory:.1f}GB")
            
            # –í—ã–±–∏—Ä–∞–µ–º GPU —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –ø–∞–º—è—Ç—å—é
            best_gpu = max(range(gpu_count), key=lambda i: torch.cuda.get_device_properties(i).total_memory)
            torch.cuda.set_device(best_gpu)
            gpu_name = torch.cuda.get_device_name(best_gpu)
            gpu_memory = torch.cuda.get_device_properties(best_gpu).total_memory / 1024**3
            logger.info(f"‚úÖ Selected GPU {best_gpu}: {gpu_name} ({gpu_memory:.1f}GB)")
            
            self.device = "cuda"
            logger.info("üöÄ GPU detected - using CUDA")
        else:
            self.device = "cpu"
            logger.info("‚ö†Ô∏è No GPU detected - using CPU")
        
        logger.info(f"üéØ Using device: {self.device}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        self.controlnets = {}
        self.has_controlnet = False
        self.use_ti = False
        self._intermediate_image = None  # –î–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SDXL pipeline —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
        logger.info("üöÄ Initializing SDXL pipeline...")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ v4.3.14 - –∏—Å–ø–æ–ª—å–∑—É–µ–º ControlNet pipeline
        try:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å ControlNet pipeline –∫–∞–∫ –≤ v4.3.14
            self.pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch.float16,
                use_safetensors=True,
                variant="fp16",
                resume_download=False
            )
            logger.info("‚úÖ SDXL ControlNet pipeline loaded successfully")
        except Exception as e:
            logger.warning(f"ControlNet pipeline failed ({e}), falling back to standard SDXL")
            # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É SDXL pipeline
            self.pipe = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch.float16,
                use_safetensors=True,
                variant="fp16",
                resume_download=False
            )
            logger.info("‚úÖ SDXL pipeline loaded successfully (fallback)")
        
        # –ü–æ–∑–∂–µ –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å ControlNet –º–æ–¥–µ–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
        # –≠—Ç–æ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —á–µ–º –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å ControlNet pipeline —Å—Ä–∞–∑—É
        
        # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU
        self.pipe = self.pipe.to(self.device)
        logger.info("‚úÖ SDXL pipeline moved to cuda")
        
        # –ó–ê–ì–†–£–ó–ö–ê –ù–ê–®–ï–ô –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò - LoRA
        logger.info("Loading OUR TRAINED LoRA model...")
        try:
            lora_path = "/src/model_files/rubber-tile-lora-v4_sdxl_lora.safetensors"
            if os.path.exists(lora_path):
                logger.info(f"‚úÖ Found LoRA file: {lora_path}")
                self.pipe.set_adapters(["rubber-tile-lora-v4_sdxl_lora"], adapter_weights=[0.7])  # –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–û: 0.7 –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π –≤–µ—Ä—Å–∏–∏ v4.3.5
                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –û–±—ä–µ–¥–∏–Ω—è–µ–º LoRA —Å –º–æ–¥–µ–ª—å—é –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
                self.pipe.fuse_lora()
                logger.info("‚úÖ OUR TRAINED LoRA loaded successfully with strength 0.7 and fused")
            else:
                logger.error(f"‚ùå OUR LoRA file NOT FOUND: {lora_path}")
                raise FileNotFoundError(f"LoRA file not found: {lora_path}")
        except Exception as e:
            logger.error(f"‚ùå Failed to load OUR LoRA: {e}")
            raise e
        
        # –ó–ê–ì–†–£–ó–ö–ê –ù–ê–®–ï–ô –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò - Textual Inversion
        logger.info("Loading OUR TRAINED Textual Inversion embeddings...")
        try:
            ti_path = "/src/model_files/rubber-tile-lora-v4_sdxl_embeddings.safetensors"
            if os.path.exists(ti_path):
                logger.info(f"‚úÖ Found TI file: {ti_path}")
                logger.info(f"üîß TI file size: {os.path.getsize(ti_path)} bytes")
                # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ Textual Inversion –¥–ª—è SDXL dual-encoder
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—á–Ω—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–±–æ—á–µ–π –≤–µ—Ä—Å–∏–∏ v4.3.14
                try:
                    # –ú–µ—Ç–æ–¥ 1: –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π v45)
                    self.pipe.load_textual_inversion(ti_path, token="<s0><s1>")
                    logger.info("‚úÖ OUR TRAINED Textual Inversion loaded successfully with standard method")
                    self.use_ti = True
                except Exception as e:
                    logger.info(f"Standard TI load not compatible ({e}). Using manual SDXL dual-encoder TI install...")
                    try:
                        # –ú–µ—Ç–æ–¥ 2: –†—É—á–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è SDXL dual-encoder (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π v45)
                        self._install_sdxl_textual_inversion_dual(ti_path, self.pipe, token_g="<s0><s1>", token_l="<s0><s1>")
                        logger.info("‚úÖ OUR TRAINED Textual Inversion loaded successfully with manual method")
                        self.use_ti = True
                    except Exception as e2:
                        logger.error(f"‚ùå Manual TI install also failed: {e2}")
                        logger.warning("‚ö†Ô∏è Continuing without Textual Inversion - using base model")
                        self.use_ti = False
            else:
                logger.error(f"‚ùå OUR TI file NOT FOUND: {ti_path}")
                raise FileNotFoundError(f"TI file not found: {ti_path}")
        except Exception as e:
            logger.error(f"‚ùå Failed to load OUR Textual Inversion: {e}")
            # Fallback: –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ Textual Inversion
            logger.warning("‚ö†Ô∏è Continuing without Textual Inversion - using base model")
            self.use_ti = False
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ ControlNet –º–æ–¥–µ–ª–µ–π
        logger.info("Loading ControlNet models...")
        
        # Canny ControlNet –¥–ª—è –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É
        try:
            self.controlnets["canny"] = ControlNetModel.from_pretrained(
                "diffusers/controlnet-canny-sdxl-1.0",
                torch_dtype=torch.float16,
                use_safetensors=True,
                resume_download=False,  # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ resume_download
                local_files_only=False  # –†–∞–∑—Ä–µ—à–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
            )
            logger.info("‚úÖ Canny ControlNet loaded")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load Canny ControlNet: {e}")
        
        # SoftEdge ControlNet –¥–ª—è —É–≥–ª–æ–≤—ã—Ö –≤–∏–¥–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å)
        try:
            self.controlnets["softedge"] = ControlNetModel.from_pretrained(
                "lllyasviel/control_v11p_sd15_softedge",
                torch_dtype=torch.float16,
                use_safetensors=True,
                resume_download=False,  # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ resume_download
                local_files_only=False  # –†–∞–∑—Ä–µ—à–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
            )
            logger.info("‚úÖ SoftEdge ControlNet loaded")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load SoftEdge ControlNet: {e}")
        
        self.has_controlnet = len(self.controlnets) > 0
        logger.info(f"ControlNet status: {self.has_controlnet} models loaded")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            self.pipe.scheduler.config,
            algorithm_type="dpmsolver++",
            use_karras_sigmas=True
        )
        logger.info("‚úÖ Scheduler configured successfully")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ VAE
        self.pipe.vae.enable_slicing()
        logger.info("‚úÖ VAE slicing enabled")
        
        self.pipe.vae.enable_tiling()
        logger.info("‚úÖ VAE tiling enabled")
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        torch.cuda.empty_cache()
        gc.collect()
        
        logger.info("üéâ OUR TRAINED MODEL setup completed successfully")
        logger.info("üéØ Using: rubber-tile-lora-v4_sdxl_lora.safetensors + rubber-tile-lora-v4_sdxl_embeddings.safetensors")
        logger.info(f"üîß Final TI Status: use_ti={self.use_ti}")
        if self.use_ti:
            logger.info("‚úÖ Textual Inversion is ENABLED and ready for generation")
        else:
            logger.warning("‚ö†Ô∏è WARNING: Textual Inversion is DISABLED - generation quality will be poor!")
    
    def _install_sdxl_textual_inversion_dual(self, ti_path: str, pipeline, token_g: str, token_l: str) -> None:
        """Install SDXL textual inversion that contains separate embeddings for CLIP-G and CLIP-L encoders."""
        try:
            # Load the safetensors file using safetensors library instead of torch.load
            # This avoids the "could not find MARK" error
            from safetensors import safe_open
            
            logger.info("üî§ Installing dual-encoder SDXL textual inversion using safetensors...")
            
            # Load embeddings using safetensors
            with safe_open(ti_path, framework="pt", device="cpu") as f:
                # Get all available keys
                available_keys = f.keys()
                logger.info(f"üî§ Available keys in TI file: {available_keys}")
                
                if 'clip_g' in available_keys and 'clip_l' in available_keys:
                    # Load embeddings
                    clip_g_embeddings = f.get_tensor('clip_g')
                    clip_l_embeddings = f.get_tensor('clip_l')
                    
                    # Determine number of tokens
                    num_tokens = clip_g_embeddings.shape[0]
                    logger.info(f"üî§ Textual inversion contains {num_tokens} token(s)")
                    
                    # Generate token names - –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
                    token_names = ["<s0>", "<s1>"]  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏
                    logger.info(f"üî§ Token names: {token_names}")
                    
                    # Install in text_encoder_2 (CLIP-G) - –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                    if hasattr(pipeline, 'text_encoder_2'):
                        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ multivariate normal distribution
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            pipeline.text_encoder_2.resize_token_embeddings(len(pipeline.tokenizer_2) + 2)  # 2 —Ç–æ–∫–µ–Ω–∞ <s0><s1>
                        with torch.no_grad():
                            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
                            pipeline.text_encoder_2.get_input_embeddings().weight[-2:] = clip_g_embeddings
                    
                    # Install in text_encoder (CLIP-L) - –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                    if hasattr(pipeline, 'text_encoder'):
                        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ multivariate normal distribution
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            pipeline.text_encoder.resize_token_embeddings(len(pipeline.tokenizer) + 2)  # 2 —Ç–æ–∫–µ–Ω–∞ <s0><s1>
                        with torch.no_grad():
                            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
                            pipeline.text_encoder.get_input_embeddings().weight[-2:] = clip_l_embeddings
                    
                    # Add tokens to tokenizers
                    if hasattr(pipeline, 'tokenizer_2'):
                        pipeline.tokenizer_2.add_tokens(token_names)
                    if hasattr(pipeline, 'tokenizer'):
                        pipeline.tokenizer.add_tokens(token_names)
                    
                    logger.info(f"‚úÖ SDXL textual inversion installed manually for {num_tokens} token(s): {token_names}")
                else:
                    # Fallback: try to load as regular embeddings
                    logger.warning("‚ö†Ô∏è Dual-encoder format not found, trying regular format...")
                    if len(available_keys) == 1:
                        key = list(available_keys)[0]
                        embeddings = f.get_tensor(key)
                        num_tokens = embeddings.shape[0]
                        
                        # Install in both encoders
                        if hasattr(pipeline, 'text_encoder_2'):
                            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ multivariate normal distribution
                            import warnings
                            with warnings.catch_warnings():
                                warnings.simplefilter("ignore")
                                pipeline.text_encoder_2.resize_token_embeddings(len(pipeline.tokenizer_2) + num_tokens)
                            with torch.no_grad():
                                pipeline.text_encoder_2.get_input_embeddings().weight[-num_tokens:] = embeddings
                        
                        if hasattr(pipeline, 'text_encoder'):
                            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ multivariate normal distribution
                            import warnings
                            with warnings.catch_warnings():
                                warnings.simplefilter("ignore")
                                pipeline.text_encoder.resize_token_embeddings(len(pipeline.tokenizer) + num_tokens)
                            with torch.no_grad():
                                pipeline.text_encoder.get_input_embeddings().weight[-num_tokens:] = embeddings
                        
                        # Add tokens - –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
                        token_names = ["<s0>", "<s1>"]  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
                        if hasattr(pipeline, 'tokenizer_2'):
                            pipeline.tokenizer_2.add_tokens(token_names)
                        if hasattr(pipeline, 'tokenizer'):
                            pipeline.tokenizer.add_tokens(token_names)
                        
                        logger.info(f"‚úÖ SDXL textual inversion installed manually for {num_tokens} token(s): {token_names}")
                    else:
                        raise ValueError(f"Textual inversion file contains unexpected keys: {available_keys}")
                
        except ImportError:
            # Fallback to torch.load if safetensors not available
            logger.warning("‚ö†Ô∏è safetensors library not available, falling back to torch.load...")
            try:
                state_dict = torch.load(ti_path, map_location="cpu", weights_only=True)
                
                if 'clip_g' in state_dict and 'clip_l' in state_dict:
                    clip_g_embeddings = state_dict['clip_g']
                    clip_l_embeddings = state_dict['clip_l']
                    num_tokens = clip_g_embeddings.shape[0]
                    
                    # Install embeddings (same logic as above)
                    if hasattr(pipeline, 'text_encoder_2'):
                        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ multivariate normal distribution
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            pipeline.text_encoder_2.resize_token_embeddings(len(pipeline.tokenizer_2) + num_tokens)
                        with torch.no_grad():
                            pipeline.text_encoder_2.get_input_embeddings().weight[-num_tokens:] = clip_g_embeddings
                    
                    if hasattr(pipeline, 'text_encoder'):
                        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ multivariate normal distribution
                        import warnings
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            pipeline.text_encoder.resize_token_embeddings(len(pipeline.tokenizer) + num_tokens)
                        with torch.no_grad():
                            pipeline.text_encoder.get_input_embeddings().weight[-num_tokens:] = clip_l_embeddings
                    
                    token_names = ["<s0>", "<s1>"]  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
                    if hasattr(pipeline, 'tokenizer_2'):
                        pipeline.tokenizer_2.add_tokens(token_names)
                    if hasattr(pipeline, 'tokenizer'):
                        pipeline.tokenizer.add_tokens(token_names)
                    
                    logger.info(f"‚úÖ SDXL textual inversion installed manually for {num_tokens} token(s): {token_names}")
                else:
                    raise ValueError("Textual inversion file does not contain dual-encoder format")
                    
            except Exception as e:
                logger.error(f"‚ùå Failed to install SDXL textual inversion: {e}")
                raise RuntimeError(f"Textual inversion installation failed: {e}")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to install SDXL textual inversion: {e}")
            raise RuntimeError(f"Textual inversion installation failed: {e}")
    
    def _build_prompt(self, colors: List[Dict[str, Any]], angle: int) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ù–ê–®–ò–• –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π v45)."""
        if not colors:
            return "ohwx_rubber_tile <s0><s1>, 100% white, photorealistic rubber tile, high quality, detailed texture, professional photography, sharp focus"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —Ü–≤–µ—Ç–æ–≤ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
        color_parts = []
        for color_info in colors:
            name = color_info.get("name", "white")
            proportion = color_info.get("proportion", 1.0)
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º proportion –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
            percentage = int(proportion * 100)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–∑ –æ–±—É—á–µ–Ω–∏—è
            color_parts.append(f"{percentage}% {name}")
        
        color_str = ", ".join(color_parts)
        
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç —Å –ù–ê–®–ò–ú–ò —Ç–æ–∫–µ–Ω–∞–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π v45)
        base_prompt = "ohwx_rubber_tile <s0><s1>"
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Textual Inversion
        # –≠—Ç–∏ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã –ù–ï–û–ë–•–û–î–ò–ú–´ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤ <s0><s1>
        quality_descriptors = [
            "photorealistic rubber tile",
            "high quality", 
            "detailed texture",
            "professional photography",
            "sharp focus"
        ]
        
        # –°–±–æ—Ä–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π v45)
        full_prompt = f"{base_prompt}, {color_str}, {', '.join(quality_descriptors)}"
        
        logger.info(f"Generated prompt exactly as in training: {full_prompt}")
        return full_prompt
    
    def _create_colormap(self, colors: List[Dict[str, Any]], size: int = 512) -> Image.Image:
        """–°–æ–∑–¥–∞–µ—Ç –∫–∞—Ä—Ç—É —Ü–≤–µ—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏."""
        img = Image.new('RGB', (size, 100), color='white')
        draw = ImageDraw.Draw(img)
        
        if not colors:
            return img
        
        # –í—ã—á–∏—Å–ª—è–µ–º —à–∏—Ä–∏–Ω—É –∫–∞–∂–¥–æ–≥–æ —Ü–≤–µ—Ç–∞
        total_width = size
        current_x = 0
        
        for color_info in colors:
            name = color_info.get("name", "white")
            proportion = color_info.get("proportion", 1.0)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è proportion –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
            percentage = int(proportion * 100)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç
            color_map = {
                "red": (255, 0, 0),
                "blue": (0, 0, 255),
                "green": (0, 255, 0),
                "yellow": (255, 255, 0),
                "white": (255, 255, 255),
                "black": (0, 0, 0),
                "orange": (255, 165, 0),
                "purple": (128, 0, 128),
                "pink": (255, 192, 203),
                "brown": (165, 42, 42),
                "gray": (128, 128, 128)
            }
            
            color_rgb = color_map.get(name.lower(), (255, 255, 255))
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —à–∏—Ä–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
            width = int((percentage / 100) * total_width)
            
            # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
            draw.rectangle([current_x, 0, current_x + width, 100], fill=color_rgb, outline=(0, 0, 0))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
            try:
                font = ImageFont.load_default()
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏
                text = f"{name} {percentage}%"
                text_bbox = draw.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_x = current_x + (width - text_width) // 2
                text_y = 40
                draw.text((text_x, text_y), text, fill=(0, 0, 0), font=font)
            except:
                # Fallback –±–µ–∑ —à—Ä–∏—Ñ—Ç–∞
                pass
            
            current_x += width
        
        return img
    
    def _save_intermediate_result(self, step: int, timestep, latents, target_step: int):
        """Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–∞ target_step."""
        if step == target_step:
            logger.info(f"üì∏ Saving intermediate result at step {step}")
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
            self._intermediate_image = None
    
    def _create_proportion_mask(self, colors: List[Dict[str, Any]], size: int = 1024) -> Image.Image:
        """–°–æ–∑–¥–∞–µ—Ç –º–∞—Å–∫—É —Å —Ç–æ—á–Ω—ã–º–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏—è–º–∏ —Ü–≤–µ—Ç–æ–≤ –¥–ª—è ControlNet."""
        mask = Image.new('L', (size, size), 0)
        draw = ImageDraw.Draw(mask)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
        total_proportion = sum(c['proportion'] for c in colors)
        current_y = 0
        
        for color_info in colors:
            proportion = color_info['proportion'] / total_proportion
            height = int(size * proportion)
            
            # –†–∏—Å—É–µ–º –æ–±–ª–∞—Å—Ç—å –¥–ª—è —Ü–≤–µ—Ç–∞
            draw.rectangle([0, current_y, size, current_y + height], fill=255)
            current_y += height
        
        logger.info(f"Created proportion mask for {len(colors)} colors")
        return mask
    
    def _apply_canny_controlnet(self, mask: Image.Image) -> Image.Image:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç Canny edge detection –∫ –º–∞—Å–∫–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–π."""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy
        mask_array = np.array(mask)
        
        # Canny edge detection
        edges = cv2.Canny(mask_array, 50, 150)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ –≤ PIL
        return Image.fromarray(edges)
    
    def _should_use_controlnet_for_proportions(self, colors: List[Dict[str, Any]]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–µ–Ω –ª–∏ ControlNet –¥–ª—è —Ç–æ—á–Ω—ã—Ö –ø—Ä–æ–ø–æ—Ä—Ü–∏–π."""
        num_colors = len(colors)
        return num_colors >= 2  # 2-4 —Ü–≤–µ—Ç–∞ = –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π ControlNet
    
    def _should_use_controlnet_for_angle(self, angle: int) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–µ–Ω –ª–∏ ControlNet –¥–ª—è —É–≥–ª–∞ –æ–±–∑–æ—Ä–∞."""
        return angle > 0  # –õ—é–±–æ–π —É–≥–æ–ª –∫—Ä–æ–º–µ 0¬∞ —Ç—Ä–µ–±—É–µ—Ç ControlNet
    
    def _generate_simple(self, prompt: str, steps: int, guidance_scale: float, seed: int) -> Any:
        """–ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞ (–±—ã—Å—Ç—Ä–æ)."""
        logger.info(f"üîß Simple generation: {steps} steps, guidance_scale={guidance_scale}")
        
        result = self.pipe(
            prompt=prompt,
            negative_prompt="blurry, worst quality, low quality, deformed, watermark, 3d render, cartoon, abstract",  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª–Ω—ã–π negative prompt –∫–∞–∫ –≤ v45
            num_inference_steps=steps,
            guidance_scale=guidance_scale,
            width=1024,
            height=1024,
            generator=torch.Generator(device=self.device).manual_seed(seed),
            # Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            callback=lambda step, timestep, latents: self._save_intermediate_result(step, timestep, latents, steps // 2),
            callback_steps=steps // 2  # Callback –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        )
        
        return result
    
    def _generate_with_controlnet(self, prompt: str, colors: List[Dict[str, Any]], angle: int, steps: int, guidance_scale: float, seed: int) -> Any:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å ControlNet –¥–ª—è —Ç–æ—á–Ω—ã—Ö –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∏ —É–≥–ª–æ–≤."""
        logger.info(f"üîß ControlNet generation: {steps} steps, guidance_scale={guidance_scale}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –¥–ª—è 2+ —Ü–≤–µ—Ç–æ–≤
        if len(colors) >= 2:
            logger.info("üé® Creating proportion mask for ControlNet")
            proportion_mask = self._create_proportion_mask(colors)
            control_image = self._apply_canny_controlnet(proportion_mask)
        else:
            # –î–ª—è –æ–¥–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞ –∏–ª–∏ —É–≥–ª–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–∞—Å–∫—É
            control_image = Image.new('L', (1024, 1024), 128)
        
        # –í—ã–±–æ—Ä ControlNet –º–æ–¥–µ–ª–∏
        if angle in [45, 135]:
            controlnet_model = self.controlnets.get("softedge")
            controlnet_type = "softedge"
        elif angle == 90:
            controlnet_model = self.controlnets.get("canny")
            controlnet_type = "canny"
        else:
            controlnet_model = self.controlnets.get("canny")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            controlnet_type = "canny"
        
        if controlnet_model is None:
            logger.warning("‚ö†Ô∏è ControlNet model not available, falling back to simple generation")
            return self._generate_simple(prompt, steps, guidance_scale, seed)
        
        logger.info(f"üîß Using {controlnet_type} ControlNet for angle {angle}¬∞")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å ControlNet (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ –≤ v45)
        result = self.pipe(
            prompt=prompt,
            negative_prompt="blurry, worst quality, low quality, deformed, watermark, 3d render, cartoon, abstract",  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª–Ω—ã–π negative prompt –∫–∞–∫ –≤ v45
            image=control_image,
            controlnet_conditioning_scale=0.8,
            num_inference_steps=steps,
            guidance_scale=guidance_scale,
            width=1024,
            height=1024,
            generator=torch.Generator(device=self.device).manual_seed(seed),
            # Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            callback=lambda step, timestep, latents: self._save_intermediate_result(step, timestep, latents, steps // 2),
            callback_steps=steps // 2  # Callback –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        )
        
        return result
    
    def predict(
        self,
        params_json: str = Input(description="JSON string with generation parameters"),
    ) -> List[Path]:
        """Generate rubber tile images using OUR TRAINED MODEL."""
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            logger.info(f"Received params_json: {params_json}")
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–≤–æ–π–Ω–æ–≥–æ –≤–ª–æ–∂–µ–Ω–∏—è JSON
            try:
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω—ã–π JSON
                params = json.loads(params_json)
                logger.info(f"Parsed as direct JSON: {params}")
            except json.JSONDecodeError:
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥–≤–æ–π–Ω–æ–µ –≤–ª–æ–∂–µ–Ω–∏–µ
                if params_json.startswith('{"params_json"'):
                    logger.info("Detected nested params_json, parsing inner JSON")
                    try:
                        inner_data = json.loads(params_json)
                        inner_json = inner_data.get("params_json", "")
                        if inner_json:
                            params = json.loads(inner_json)
                            logger.info(f"Extracted and parsed inner JSON: {params}")
                        else:
                            raise ValueError("Inner params_json is empty")
                    except (json.JSONDecodeError, ValueError) as e:
                        logger.error(f"Failed to parse nested JSON: {e}")
                        raise ValueError(f"Invalid nested JSON format: {e}")
                else:
                    raise ValueError("Invalid JSON format")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if not params:
                raise ValueError("Empty parameters")
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ü–≤–µ—Ç–æ–≤
            colors = params.get("colors", [{"name": "white", "proportion": 1.0}])
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ –Ω–æ–≤—ã–π
            converted_colors = []
            for color_info in colors:
                # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞: {"name": "red", "proportion": 0.7}
                if "name" in color_info and "proportion" in color_info:
                    converted_colors.append({
                        "name": color_info["name"],
                        "proportion": float(color_info["proportion"])
                    })
                # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞: {"color": "red", "percentage": 70}
                elif "color" in color_info and "percentage" in color_info:
                    converted_colors.append({
                        "name": color_info["color"],
                        "proportion": float(color_info["percentage"]) / 100.0
                    })
                # Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
                else:
                    logger.warning(f"Unknown color format: {color_info}, using default")
                    converted_colors.append({"name": "white", "proportion": 1.0})
            
            colors = converted_colors
            angle = int(params.get("angle", 0))
            quality = str(params.get("quality", "standard"))
            seed = int(params.get("seed", -1))
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è seed
            if seed == -1:
                logger.info("üé≤ Seed not provided, will use random")
            elif seed < 0:
                logger.warning(f"Invalid seed {seed}, using random")
                seed = -1
            else:
                logger.info(f"üéØ Using provided seed: {seed}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            logger.info(f"Converted colors: {colors}")
            logger.info(f"Extracted angle: {angle} (type: {type(angle)})")
            logger.info(f"Extracted quality: {quality} (type: {type(quality)})")
            logger.info(f"Extracted seed: {seed} (type: {type(seed)})")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
            if not isinstance(colors, list) or len(colors) == 0:
                raise ValueError("Colors must be a non-empty list")
            
            if not isinstance(angle, int) or angle < 0 or angle > 360:
                raise ValueError(f"Invalid angle: {angle}, must be 0-360")
            
            if quality not in ["preview", "standard", "high"]:
                raise ValueError(f"Invalid quality: {quality}, must be preview/standard/high")
            
            logger.info(f"Generating with OUR model - params: colors={len(colors)}, angle={angle}, quality={quality}, seed={seed}")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π v45)
            quality_settings = {
                "preview": {"steps": 20, "guidance_scale": 7.0},  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: 20 —à–∞–≥–æ–≤ –∫–∞–∫ –≤ v45
                "standard": {"steps": 20, "guidance_scale": 7.0},  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: 20 —à–∞–≥–æ–≤ –∫–∞–∫ –≤ v45
                "high": {"steps": 35, "guidance_scale": 7.0}  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: 35 —à–∞–≥–æ–≤ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            }
            
            settings = quality_settings.get(quality, quality_settings["standard"])
            steps = settings["steps"]
            guidance_scale = settings["guidance_scale"]
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç —Å –ù–ê–®–ò–ú–ò —Ç–æ–∫–µ–Ω–∞–º–∏
            prompt = self._build_prompt(colors, angle)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ seed
            if seed == -1:
                random_seed = random.randint(0, 999999999)
                logger.info(f"üé≤ Using random seed: {random_seed}")
                torch.manual_seed(random_seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(random_seed)
                    torch.cuda.manual_seed_all(random_seed)
            else:
                logger.info(f"üéØ Using provided seed: {seed}")
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(seed)
                    torch.cuda.manual_seed_all(seed)
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É —Ü–≤–µ—Ç–æ–≤
            logger.info(f"Building color map for {len(colors)} colors")
            colormap = self._create_colormap(colors)
            colormap_path = "/tmp/colormap.png"
            colormap.save(colormap_path)
            logger.info(f"Color map saved to {colormap_path}")
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –û–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–≤—É–º—è —Ç–æ—á–∫–∞–º–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            # 20 —à–∞–≥–æ–≤ ‚Üí –ø—Ä–µ–≤—å—é, 40 —à–∞–≥–æ–≤ ‚Üí —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ
            logger.info(f"üöÄ Starting optimized generation: {steps * 2} steps total")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ ControlNet
            use_controlnet = False
            if self.has_controlnet and angle > 0:
                if angle == 45 and "softedge" in self.controlnets:
                    use_controlnet = True
                    controlnet_type = "softedge"
                elif angle == 90 and "canny" in self.controlnets:
                    use_controlnet = True
                    controlnet_type = "canny"
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ù–ê–®–ï–ô –º–æ–¥–µ–ª—å—é (–æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥)
            if self.use_ti:
                logger.info("üîß Generation with OUR TRAINED MODEL (TI enabled)")
                logger.info(f"üîß TI Status: use_ti={self.use_ti}")
            else:
                logger.info("üîß Generation with OUR TRAINED MODEL (TI disabled, LoRA only)")
                logger.warning("‚ö†Ô∏è WARNING: Textual Inversion is DISABLED - this will cause poor generation quality!")
            
            # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø: –û–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ —Å callback –¥–ª—è preview
            logger.info("üé® Starting optimized generation with callback")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ ControlNet
            use_controlnet_proportions = self._should_use_controlnet_for_proportions(colors)
            use_controlnet_angle = self._should_use_controlnet_for_angle(angle)
            
            if use_controlnet_proportions or use_controlnet_angle:
                logger.info("üîß Using ControlNet for precise control")
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å ControlNet
                final_result = self._generate_with_controlnet(
                    prompt=prompt,
                    colors=colors,
                    angle=angle,
                    steps=steps,  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±—Ä–∞–ª–∏ —É–¥–≤–æ–µ–Ω–∏–µ —à–∞–≥–æ–≤
                    guidance_scale=guidance_scale,
                    seed=seed
                )
            else:
                logger.info("üîß Using simple generation for single color")
                # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞
                final_result = self._generate_simple(
                    prompt=prompt,
                    steps=steps,  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±—Ä–∞–ª–∏ —É–¥–≤–æ–µ–Ω–∏–µ —à–∞–≥–æ–≤
                    guidance_scale=guidance_scale,
                    seed=seed
                )
            
            # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
            # –°–æ–∑–¥–∞–µ–º preview –∏–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            preview_path = "/tmp/preview.png"
            if hasattr(self, '_intermediate_image') and self._intermediate_image is not None:
                self._intermediate_image.save(preview_path)
                logger.info("üîß Preview extracted from intermediate step")
            else:
                # Fallback: —Å–æ–∑–¥–∞–µ–º preview –∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                final_image = final_result.images[0]
                preview_image = final_image.resize((512, 512))
                preview_image.save(preview_path)
                logger.info("üîß Preview created from final image (resized)")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            final_image = final_result.images[0]
            final_path = "/tmp/final.png"
            final_image.save(final_path)
            logger.info("üîß Final image saved")
            
            logger.info("üöÄ Optimized generation completed successfully (one pass, two outputs)")
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            torch.cuda.empty_cache()
            gc.collect()
            
            generation_time = 0  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
            logger.info(f"‚úÖ Generation with OUR TRAINED MODEL completed in {generation_time:.2f}s")
            
            # Return the generated images as Path objects
            # Replicate will automatically convert Path objects to URLs
            return [
                Path(preview_path),
                Path(final_path), 
                Path(colormap_path)
            ]
            
        except Exception as e:
            logger.error(f"‚ùå Generation with OUR MODEL failed: {e}")
            raise e
